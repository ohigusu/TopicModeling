{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysrC0j6TjXHg"
      },
      "source": [
        "# 1ï¸âƒ£ì‚¬ì „ ì •ì˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ5A7FyJjWdr"
      },
      "source": [
        "## ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „\n",
        "ëª…ì‚¬ ë¶„ë¥˜ê°€ ì˜ ì•ˆë˜ëŠ” ê·¼ë³¸ì ì¸ ì›ì¸ì€ ë¶„ë¥˜í•˜ë ¤ëŠ” ì–¸ì–´ê°€ ë¶í•œì–´ì´ê¸° ë•Œë¬¸ì´ë‹¤. íŠ¹íˆ, ë¶í•œì—ëŠ” ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìš©ì–´ì—ëŠ” í•œêµ­ê³¼ ë‹¤ë¥¸ì ì´ ë§ë‹¤. ë”°ë¼ì„œ, ë¶í•œì •ë¶€í¬í„¸ì—ì„œ ì œê³µí•˜ëŠ” ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœì— ëŒ€í•œ ì„¤ëª…ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ê² ë‹¤.\n",
        "\n",
        "ì°¸ê³ í•œ ì‚¬ì´íŠ¸:https://nkinfo.unikorea.go.kr/nkp/pge/ps/jung.do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0IMcb9ljiXz"
      },
      "outputs": [],
      "source": [
        "#ì‚¬ì „ ì •ì˜\n",
        "PoliticalTerms = ['ì‚¬íšŒì£¼ì˜','ì¡°ì„ ë…¸ë™ë‹¹','ìˆ˜ë ¹','ì¥êµ°','ìš°ë¦¬ì‹','ìš°ë¦¬ ì‹','ìš°ë¦¬ë‹¹','ìš°ë¦¬ ë‹¹','ìš°ë¦¬ë¯¼ì¡±','ìš°ë¦¬ ë¯¼ì¡±','ìš°ë¦¬ í˜','ìš°ë¦¬í˜',\"ìš°ë¦¬ë‚˜ë¼\",'ìš°ë¦¬','ë¡œë™ë‹¹','ì¡°êµ­','ê±´ì„¤ê²½ì œë ¥', 'ê²½ì œì ', 'ê²½ì œì œì¬', 'ìë¦½ê²½ì œ','êµ­ê°€ê²½ì œ',\n",
        "                      'ì¡°ì§ì§€ë„ë¶€','ì„ ì „ì„ ë™ë¶€','ê°„ë¶€ë¶€','ê²½ê³µì—…ë¶€','ê²½ì œë¶€','ê³¼í•™êµìœ¡ë¶€','êµ­ì œë¶€','êµ°ìˆ˜ê³µì—…ë¶€','êµ°ì •ì§€ë„ë¶€','ê·œìœ¨ì¡°ì‚¬ë¶€','ê·¼ë¡œë‹¨ì²´ë¶€','ë†ì—…ë¶€','ë‹¹ì—­ì‚¬ì—°êµ¬ì†Œ','ì¡°ì„ ë¡œë™ë‹¹',\n",
        "                      'ë¬¸ì„œì •ë¦¬ì‹¤','ë¬¸í™”ì˜ˆìˆ ë¶€','ë¯¼ë°©ìœ„ë¶€','ë²•ë¬´ë¶€','ì¬ì •ê²½ë¦¬ë¶€','ì´ë¬´ë¶€','10êµ­','39í˜¸ì‹¤','ê²½ì œì •ì±…ì‹¤','ê²½ì œë°œì „','ê²½ì œì‚¬ì—…','ì‚¬íšŒê²½ì œ','ê²½ì œêµ¬ì¡°','ì§€ë°©ê²½ì œ','ê²½ì œí™œì„±í™”',\n",
        "                      'ìµœê³ ì¸ë¯¼íšŒì˜','êµ­ë¬´ìœ„ì›íšŒ','ë‚´ê°','ì‚¬ë²•ê²€ì°°ê¸°ê´€','ì¤‘ì•™ì¬íŒì†Œ', 'ì¤‘ì•™ê²€ì°°ì†Œ', 'ì¸ë¯¼ì •ê¶Œ','ì „ì›íšŒì˜','ë ¹ë„','ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­','ì¡°ì„ ì¸ë¯¼êµ°','ìˆ˜ë ¹','ë¦¬ìµ',\n",
        "                      'í˜ëª…ì‚¬ìƒ','ë¡œë™ê³„ê¸‰','ê·¼ë¡œì¸ë¯¼ëŒ€ì¤‘','í•µì‹¬ë¶€ëŒ€','ì „ìœ„ë¶€ëŒ€','ìˆ˜ë ¹ì²´ì œ','ì¸ë¯¼ëŒ€ì¤‘','ê·¼ë¡œëŒ€ì¤‘','ì¤‘ì•™ì§‘ê¶Œì œ','ìƒì˜í•˜ë‹¬','ì¡°ì§ìƒí™œ','ì‚¬ìƒìƒí™œ','ì¡°ì§ì§€ë„ë¶€','ì„ ì „ì„ ë™ë¶€',\n",
        "                      'ìœ ì¼ì§€ë°°','ì´ë…','ë‹¹ë¬´','í–¥ë„ì','ìµœê³ ì§€ë„ì','ì „ê¶Œ','ì¡°ì„ ë¡œë™ë‹¹ëŒ€íšŒ','ë‹¹ ëŒ€íšŒ','ë‹¹ëŒ€íšŒ','ë‹¹ ëŒ€í‘œìíšŒ','ë‹¹ëŒ€í‘œìíšŒ','ìµœê³ ì°¸ëª¨ë¶€','ì°¸ëª¨ë¶€'\n",
        "                      'ì˜ì‚¬ê²°ì •','ë…¸ì„ ','ì •ì±…',\"ì „ëµì „ìˆ \",'í†µì¼','ì •ì¹˜êµ­','ì •ì¹˜ë…¸ì„ ', 'ì¡°ì§ë…¸ì„ ','ì¡°ì„ ë¡œë™ë‹¹ ìœ„ì›ì¥ë™ì§€','í™”í•™','ì „ë ¥','êµìœ¡','ê±´ì„¤','ë‚˜ë¼','ï¼–.ï¼‘ï¼•',\n",
        "                      'ì¤‘ì•™ì§€ë„ê¸°ê´€','ë‹¹ì¤‘ì•™ì§€ë„ê¸°ê´€','ì¤‘ì•™ê¸°ê´€','ë‹¹ì¤‘ì•™ê¸°ê´€','ì¤‘ì•™ê²€ì‚¬ìœ„ì›íšŒ','ë‹¹ ì¤‘ì•™ê²€ì‚¬ìœ„ì›íšŒ','ë‹¹ì¤‘ì•™ê²€ì‚¬ìœ„ì›íšŒ','ë‹¹ì¤‘ì•™ìœ„ì›íšŒ','ë‹¹ ì¤‘ì•™ìœ„ì›íšŒ','ì¤‘ì•™ìœ„ì›íšŒ',\n",
        "                      'ìƒë¬´ìœ„ì›','ìµœê³ ìˆ˜ìœ„','ìœ„ì›ì¥','ì´ë¹„ì„œ','ë¹„ì„œ','ë‹¹ëŒ€í‘œìíšŒ',\"ë‹¹ ëŒ€í‘œìíšŒ\",'ëŒ€í‘œìíšŒ', 'ë‹¹ê·œì•½','ì¸ë¯¼ê²½ì œë°œì „', 'ì •ì„¸', 'ê³¼ì—…','êµ­ë°©','ê²½ì œ','ë³‘ì§„ì •ì±…','ê²½ì œê±´ì„¤','ì›”ë‚¨ë¬¸ì œ',\n",
        "                      'ë‹¹ ì¤‘ì•™ìœ„ì›ì¥ì œ','ë‹¹ì¤‘ì•™ìœ„ì›ì¥ì œ','ì¤‘ì•™ìœ„ì›ì¥ì œ','ì§ì œ','ìˆ˜ë ¹','ì œ1ë¹„ì„œì§','ì¤‘ì•™êµ°ì‚¬ìœ„ì›íšŒ','ë‹¹ì¤‘ì•™êµ°ì‚¬ìœ„ì›íšŒ','ë‹¹ ì¤‘ì•™êµ°ì‚¬ìœ„ì›íšŒ','ìµœê³ ì§€ë„ê¸°ê´€','ëŒ€í–‰', 'í›„ë³´ìœ„ì›',\n",
        "                      'ë‚´ì™¸',\"ë…¼ì˜\",'ì˜ê²°','ê¶Œë ¥ê¸°êµ¬','ì•ˆê±´','í™•ëŒ€íšŒì˜','ì •ë¬´êµ­','ë¹„ì„œì œ','êµ°ì‚¬ë…¸ì„ ',\"ê³µí™”êµ­\",'ë¬´ë ¥','ì§€íœ˜','êµ°ìˆ˜ê³µì—…','êµ­ë°©ì‚¬ì—…','êµ­ë°©ë ¥','í˜„ëŒ€í™”','ì¡°ì§ë¹„ì„œ',\n",
        "                      'ë‚´ê°ì´ë¦¬','ì£¼ê¶Œê¸°ê´€','í—Œë‚©ìš´ë™','ì¡°ì„ ì†Œë…„ë‹¨','í—Œë‚©','íˆ¬ìŸ','ê³„íš','êµ­ë°©ê³¼í•™ë°œì „','êµ­ì œì •ì„¸','êµ­ìœ„','êµ­ìµ','ë¶ë‚¨ê´€ê³„','ëŒ€ë¯¸ëŒ€ì ','ë°˜ê³µí™”êµ­','ëŒ€ë¯¸','í•µ',\n",
        "                      'ë¯¸êµ­','ë¶ë‚¨','ëŒ€ì ì‚¬ì—…','ì¡°ì„ ë°˜ë„','ì„ ë°•ê³µì—…','êµ­ë°©ê³µì—…','ë¯¼ë°©ìœ„ë¬´ë ¥','í•¨ì„ ê³µì—…','ë¬´ì¸í•­ê³µê³µì—…','ë¬´ì¸ë¬´ì¥ì¥ë¹„','ë¬´ì¸','ì •ì°°ìœ„ì„±','ìš°ì£¼ê³¼í•™','ë¯¸ì‹¸ì¼','ë¬´ì¥ì¥ë¹„','ëŒ€ì‚¬ë³€',\n",
        "                      'í•µë¬´ê¸°ìƒì‚°ê³„íš','ë‚¨ì¡°ì„ ','í•µìœ„ê¸°ì‚¬íƒœ','ì•ˆë³´','ë¬´ê¸°ì²´ê³„ê°œë°œê³„íš','êµ°ì‚¬','ì¸ë¯¼êµ°ëŒ€','9.19ë¶ë‚¨êµ°ì‚¬ë¶„ì•¼í•©ì˜','ë¡œê³¨í™”','ìœ ì—”êµ°ì‚¬ë ¹ë¶€','ì‚¬íšŒì£¼ì˜ê°•ì„±êµ­ê°€ê±´ì„¤','ì¸ë¯¼êµ°ì¥ë³‘','ì¡°êµ­í†µì¼ìœ„ì—…','ì¸ë¯¼ê²½ì œ'\n",
        "                      'ê´´ë¢°íŒ¨ë‹¹','ê´´ë¢°êµ°ë¶€','ê´´ë¢°êµ°ë¬´ë ¥','ê´´ë¢°ì •ê¶Œ','ë§ë™','ë‚´ê°ë¶€ì´ë¦¬','ì¹¨ëµì „ìŸê¸°ë„','êµ°ì‚¬ë¶„ê³„ì„ ì§€ì—­','ë†ì—…ìœ„ì›íšŒ','ëŒ€í•œë¯¼êµ­','êµ­ê°€ì˜ˆì‚°','ì˜ˆì‚°','ëŒ€íšŒ','ìœ¤ì„ì—´',\n",
        "                      'ì¡°êµ­í†µì¼ë¡œì„ ','ëŒ€ë¶','í†µì¼','í¡ìˆ˜í†µì¼','ë¬¸ì¬ì¸','ììœ ë¯¼ì£¼ì£¼ì˜','ë¯¼ì£¼ì£¼ì˜','í™”ì„±-18í˜•','ì‹ í˜•','ê³ ì²´ì—°ë£Œ','ì—”ì§„','ì¤‘ê±°ë¦¬','íƒ„ë„ë¯¸ì‚¬ì¼','ì¤‘ëŒ€ê³¼ì—…','ë‹¹ ì§€ë°©ì¡°ì§','ì§€ë°©ì¡°ì§',\n",
        "                      'ì§‘ê¶Œì²´ì œ','ì¡°ì„ í˜ëª…','í˜ëª…','ë‹¹ ìœ„ì›íšŒ','ìƒí•˜','ìœ„ê³„','ì—¬íƒ€','ê¸°ê´€','ì‚¬íšŒë‹¨ì²´','ì§€ë°°ë ¥','í–‰ì‚¬','ë„,ì‹œ,êµ°','ì„±,ë„,ì‹œ,êµ°','ë„,ì‹œ,êµ°ì¸ë¯¼íšŒ','ë„ ë‹¹ ìœ„ì›íšŒ', 'ì‹œ ë‹¹ ìœ„ì›íšŒ', 'êµ° ë‹¹ ìœ„ì›íšŒ', 'ì´ˆê¸‰ë‹¹', 'ë¶„ì´ˆê¸‰ë‹¹', 'ë¶€ë¬¸ë‹¹', 'ë‹¹ì›','ìµœí•˜','ê¸°ì¸µì¡°ì§',\n",
        "                      'ë‹¹ì„¸í¬','ê´€í• ì§€ì—­','ì¤‘ì•™ë‹¹','í•˜ë¶€','ì¡°ì§ì²´ê³„','ì‹œ ë„ ë‹¹ìœ„ì›íšŒ','ë„,ì‹œ','ì‹œ,êµ°','êµ­ê°€','ì¸ë¯¼','ì‹œ êµ° ë‹¹ìœ„ì›íšŒ', 'ì´ˆê¸‰ ë‹¹ìœ„ì›íšŒ', 'ë‹¹ ì„¸í¬ì¡°ì§','ì„¸í¬ì¡°ì§', 'ì§‘í–‰ìœ„ì›íšŒ','ë¹„ì„œì²˜', ' ì • ', ' êµ° ',' ë„ ',' ì‹œ ',' ì„± ',\n",
        "                      'ì •ê¶Œê¸°ê´€','ì…ë²•','ì§‘í–‰','ì¸ì‚¬','ì§ì±…','ë³´ì„','ê²¸ì§' ,'í–‰ì •','ì§€ìœ„,''êµ°ì‚¬ë…¸ì„ ', 'í† ì˜','ì „ë°˜','êµ°ëŒ€','ì •ì¹˜ìœ„ì›','ì¸ë¯¼êµ°','ì´ì •ì¹˜êµ­','ì§‘í–‰ë¶€ì„œ','ì™¸ê³½ë‹¨ì²´','ì‚¬íšŒì£¼ì˜ì• êµ­ì²­ë…„ë™ë§¹','ì¡°ì„ ì§ì—…ì´ë™ë§¹',\n",
        "                      'ì¡°ì„ ë†ì—…ê·¼ë¡œìë™ë§¹','ì¡°ì„ ì‚¬íšŒì£¼ì˜ì—¬ì„±ë™ë§¹','ì‚¬ìƒêµì–‘','ì „ìœ„ëŒ€','ì˜ë„','êµ­ë¬´ìœ„ì›íšŒ','ìµœê³ ì£¼ê¶Œê¸°ê´€', 'ìµœê³ ì¸ë¯¼íšŒì˜', 'ë‚´ê°','ì¤‘ì•™ê²€ì°°ì†Œ','ì¤‘ì•™ì¬íŒì†Œ','ì‚¬ë²•ê¸°ê´€','êµ­ë¬´ìœ„ì›ì¥',\n",
        "                      'êµ­ë¬´ìœ„ì›íšŒ','ì •ë ¹', 'ì´ë¦¬','ë¶€ì´ë¦¬', 'ìœ„ì›ì¥','ì„ëª…','í•´ì„','ìµœê³ ì˜ë„ì','ì´ì‚¬ë ¹ê´€','íŠ¹ì‚¬ê¶Œ','êµ­ê°€ë°©ìœ„ìœ„ì›íšŒ','êµ­ë°©ìœ„ì›íšŒ','ì¶”ëŒ€','ì¬ì¶”ëŒ€','ì…ë²•ê¶Œ','ìµœê³ ì£¼ê¶Œê¸°ê´€',\n",
        "                      'ì •ê¸°íšŒì˜', \"ì„ì‹œíšŒì˜\",'ìƒì„ìœ„ì›íšŒ','ëŒ€ì˜ì›','ë²•ë ¹','ëŒ€ë‚´ì™¸' ,'ì œì˜','ì œ1ë¶€ìœ„ì›ì¥', 'ìœ„ì›','ì„ ì¶œ','ìœ„ì›ì¥', 'ë¶€ìœ„ì›ì¥', 'ì„œê¸°ì¥',\"ì§ìœ„ì\",'êµ­ê°€ì˜ˆì‚°','ì‹¬ì˜','ìŠ¹ì¸','ì¡°ì•½','ë¹„ì¤€',\n",
        "                      'íê¸°ê¶Œ','ê±°ìˆ˜ê°€ê²°','ì˜ˆì‚°ìœ„ì›íšŒ', \"ë²•ì œìœ„ì›íšŒ\", 'ì™¸êµìœ„ì›íšŒ','ë¶€ë¬¸ìœ„ì›íšŒ','ì •ì±…ì•ˆ','ë²•ì•ˆ','íœ´íšŒ','ìƒì„ìœ„ì›íšŒ', 'ë³´ì¶©ì•ˆ','ì‹¬ì˜', 'êµ­íšŒ','êµ­ì œì˜íšŒê¸°êµ¬','ì‹ ì„ì¥',\"ì†Œí™˜ì¥\",'ìƒë¬´íšŒì˜',\n",
        "                      'êµ­ê°€ê´€ë¦¬','êµ­ë°©','ì§‘í–‰ê¸°ê´€','ê´€ë¦¬ê¸°ê´€','ì •ë¬´ì›','êµ­ê°€ì£¼ì„','ê²€ì°°ê¸°ê´€','ê²€ì°°','ê²€ì°°ì†Œ','íŠ¹ë³„ê²€ì°°ì†Œ','í•˜ê¸‰ê²€ì°°ì†Œ','ìƒê¸‰ê²€ì°°ì†Œ','ì¤‘ì•™ê²€ì°°ì†Œì¥','ì¸ë¯¼ì¬íŒì†Œ','íŠ¹ë³„ì¬íŒì†Œ','ì¸ë¯¼ì°¸ì‹¬ì›',\n",
        "                      'ë°°ì‹¬ì›','ìµœê³ ì¬íŒê¸°ê´€','ì¤‘í•µ','í•µë¬´ì¥ë ¥ í•µì‹œí—˜','í•µìœ„í˜‘','ë¹„í•µí™”','í•µë¬´ê¸°','í•µì „ìŸ','í•µë¬´ë ¥','ì—´í•µë¬´ê¸°','í•µê°•êµ­','í•µë°˜ê²©','í•µì–µì œë ¥','í•µíƒ€ê²©','í•µíƒ„ë‘','ì¡°êµ­í†µì¼','í†µì¼ì ','í‰í™”í†µì¼','í†µì¼ë°©ìš´',\n",
        "                      'ë‚¨ë¶í†µì¼','ìì£¼í†µì¼','í†µì¼ìš´ë™','í†µì¼ëŒ€íšŒ','í†µì¼ëŒ€ì§„êµ°','ë‚¨ì¡°ì„ ','ë‚¨ì¡°ì„ ê²ƒë“¤','ë¦¬ë…','ë¦¬ìƒ','ê²½ì œë¶€','ë†ì—…ë¶€','ìµœê³ íšŒì˜','ë°±ë‘ì‚°','í•´ì™¸ë™í¬','ì• êµ­í—Œì‹ ','ë ¥ì‚¬ì ','ë ¥ì‚¬','ì „íˆ¬', 'ëŒíŒŒì „','ì§€êµ¬ê´€ì¸¡ìœ„ì„±','ê´‘ëª…ì„±-4','ì •ì§€ìœ„ì„±','ìš´ë°˜ë¡œì¼“ìš©','ì§€ìƒë¶„ì¶œì‹œí—˜',\n",
        "                      'ìš°ì£¼ì •ë³µ','ê³¼í•™ì—°êµ¬','ë…¸ë†ì ìœ„êµ°','2Â·8ë¹„ë‚ ë¡ ì—°í•©ê¸°ì—…ì†Œ','ì „ë¯¼ì´ëŒê²©ì „','ë°•ê·¼í˜œ','ì•„ì‹œì•„íƒœí‰ì–‘ì§€ë°°ì „ëµ','ì œêµ­ì£¼ì˜' ,'ë°˜ë™ì„¸ë ¥','ë¶ê³¼ ë‚¨','êµ­ë°©ë¶„ì•¼','ê³¼í•™êµìœ¡','ê³¼í•™ê¸°ìˆ ì„±ê³¼',\n",
        "                      'ìˆ˜ì†Œí­íƒ„','ìˆ˜ì†Œíƒ„','ëŒ€ë¥™ê°„ íƒ„ë„ë¡œì¼“','êµ­ë°©ë ¥','í•µíƒ„ë‘','7Â·4ê³µë™ì„±ëª…','ì¡°êµ­í†µì¼3ëŒ€ì›ì¹™','6.15ê³µë™ì„ ì–¸','6.15ê³µë™','10.4ì„ ì–¸',\"6Â·15ê³µë™ì„ ì–¸\",'ï¼‘ï¼.ï¼”ì„ ì–¸ë°œí‘œ','ï¼‘ï¼.ï¼”ì„ ì–¸ë°œí‘œ','ï¼‘ï¼.ï¼”','ï¼–.ï¼‘ï¼•ê³µë™ì„ ì–¸', '10Â·4ì„ ì–¸','í†µì¼í—Œì¥','í†µì¼ëŒ€ê°•','ì „ë‹¹ì´ˆê¸‰ë‹¹ìœ„ì›ì¥ëŒ€íšŒ','ì´ˆê¸‰ë‹¹ì¡°ì§','ì•„ì‹œì•„íƒœí‰ì–‘ì§€ë°°ì „ëµ','ëŒ€ì•„ì‹œì•„ì§€ë°°ì „ëµ','ë°˜ê³µí™”êµ­ì „ìŸì±…ë™',\n",
        "                      'ë‚¨ì¡°ì„ ë‹¹êµ­','ë¶ë‚¨ë‹¹êµ­','ë‚¨ì¡°ì„ í˜¸ì „ê´‘','ì²­ë…„ë™ë§¹','ì •ì „í˜‘ì •','í‰í™”í˜‘ì •','ì–´ë¨¸ë‹ˆë‹¹','ì„¸ê³„í‰í™”','ë¡œì”¨ì•¼','ê°•ì„±ëŒ€êµ­ê±´ì„¤','ë ¹ë„','ì˜ë„','ì–´ë²„ì´','ê¹€ì •ì€','ì–´ë²„ì´ìˆ˜ë ¹','ì–´ë²„ì´ì¥êµ°ë‹˜','ì§ë§¹', 'ë†ê·¼ë§¹', 'ì—¬ë§¹ ì¡°ì§','ì„ ë¦°ìš°í˜¸','ì¹œì„ í˜‘ì¡°ê´€ê³„','ì¹œì„ í˜‘ì¡°','í†µì¼ëŒ€íšŒí•©',\n",
        "                      'ì‚¬ëŒ€ë§¤êµ­','ë™ì¡±ëŒ€ê²°','êµ°ì¸','ì‚¬ëŒ€ë§¤êµ­ì±…ë™','ì„¸ê³„í‰í™”','í‰í™”ë²ˆì˜','ì „ì„±ê¸°','ì¤‘êµ­','ë¯¸êµ­','ì™¸ì„¸','ë¯¸ì œì¹¨ëµêµ°','ë°±ë‘ì‚°ì˜ì›…ì²­ë…„ë°œì „ì†Œ','ì²­ì²œê°•ê³„ë‹¨ì‹ë°œì „ì†Œ','ê³¼í•™ê¸°ìˆ ì „ë‹¹','ë¯¸ë˜ê³¼í•™ìê±°ë¦¬','ì¥ì²œë‚¨ìƒˆì „ë¬¸í˜‘ë™ë†ì¥','ì‚¬ìƒê´€ì² ì „',\n",
        "                      'ë‹¹ì •ì±…ì˜¹ìœ„ì „','ê²½ì œê°•êµ­ê±´ì„¤','í˜‘ë™ë†ì¥','ë¡œë†ì ìœ„êµ°','ì¸ê³µì§€êµ¬ìœ„ì„±','ë°˜ê³µí™”êµ­ì œì¬','ë¶ì¹¨ì „ìŸì†Œë™','ìœ„ì„±ê³¼í•™ìì£¼ì±…ì§€êµ¬', 'ê¹€ì±…ê³µì—…ì¢…í•©ëŒ€í•™','ì—°í’ê³¼í•™ìíœ´ì–‘ì†Œ','ì˜¤ì¤‘í¡7ì—°ëŒ€ ì¹­í˜¸ìŸì·¨ìš´ë™', 'ê·¼ìœ„ë¶€ëŒ€ìš´ë™','ì •ì¹˜ì‚¬ìƒì‚¬ì—…','ì •ì¹˜ì‚¬ìƒê°•êµ­','ì• êµ­ì‚¬ì—…',\n",
        "                      'ì• êµ­í—Œì‹ ','ë™í¬','ë™ì§€','ê¹€ì¼ì„±-ê¹€ì •ì¼ì£¼ì˜ì','ê¹€ì¼ì„±','ê¹€ì •ì¼','ì¡°êµ­í†µì¼','ì¥ë³‘','êµ­ë°©ë ¥','êµ°ëŒ€',' êµ° ','ì¡°êµ­í•´ë°©ì „ìŸìŠ¹ë¦¬ê¸°ë…ê´€', 'ì€í•˜ê³¼í•™ìê±°ë¦¬', 'ë¬¸ìˆ˜ë¬¼ë†€ì´ì¥', 'ë§ˆì‹ë ¹ìŠ¤í‚¤ì¥',\"ì „ëµì „ìˆ ì ë°©ì¹¨\",\n",
        "                      'ï¼–.ï¼‘ï¼’ì¡°ë¯¸ê³µë™ì„±ëª…','ì¡°ë¯¸ìˆ˜ë‡Œìƒë´‰', 'ì¡°ë¯¸','íŒë¬¸ì ì„ ì–¸','ï¼™ì›”í‰ì–‘ê³µë™ì„ ì–¸','ì„íƒ„','ë¬¸ëª…ê°œí™”ê¸°','ê²¨ë¡€','ë‚¨ë…˜','ëŒ€ì¡°ì„ ','ë†ê¸°ê³„','ì•Œê³¡ê³ ì§€','ë†ì¥','ê³¼í•™ê¸°ìˆ ì¤‘ì‹œê¸°í’','í˜„ëŒ€ê³¼í•™ê¸°ìˆ ','ê³¼í•™ê¸°ìˆ ','ì „ë¯¼ê³¼í•™ê¸°ìˆ ì¸ì¬í™”','ë¬¸í™”ì˜ˆìˆ ','ëª…ì‘','ê³µë™ì„ ì–¸','í’ë ¥','ìˆ˜ë ¥','ì§€ì—´','íƒœì–‘ì—´','ì—ë„¤ë¥´ê¸°','ìì—°ì—ë„¤ë¥´ê¸°',\n",
        "                      'í˜„ëŒ€ì ë¬´ì¥ì¥ë¹„','í˜„ëŒ€ê³¼í•™ê¸°ìˆ ','ê³¼í•™ê¸°ìˆ ìœ„ì„±','ì „ìŸë„ë°œì±…ë™','ì‚¬íšŒì£¼ì˜ê°•ì„±êµ­ê°€','ì²œí•˜ì œì¼ê°•êµ­','ì‚¬íšŒì£¼ì˜ê°•êµ­','ê²½ì œê°•êµ­','ê²½ì œê±´ì„¤','ì‹¤ìš©ìœ„ì„±','ìš°ì£¼','ë¯¼ì¡±ê²½ì œ','ê³¼í•™í™”','ê°•ì„±êµ­ê°€','êµ°ë ¥','í˜ëª…ì ë ¹êµ°ì²´ê³„','ì¸ë¯¼êµ°ëŒ€','ì¡°ì„ ì¸ë¯¼ë‚´ë¬´êµ°','ë‚´ë¬´êµ°', 'êµ°ê¸°','êµ°í’','ìµœì •ì˜ˆí˜ëª…ê°•êµ°','ë°±ë‘ì‚°í˜ëª…ê°•êµ°','ë°±ë‘ì‚°í›ˆë ¨','ì „íˆ¬ë™ì›íƒœì„¸','êµ°ì¸','ë¬¸í™”í›„ìƒì‹œì„¤', 'ê³µì›','ìœ ì›ì§€',\n",
        "                      'ê·¼ë¡œë‹¨ì²´ì¡°ì§','ì¼ê¾¼','ê·¼ë¡œì¸ë¯¼ëŒ€ì¤‘','ì •ì¹˜ì‚¬ìƒ','ê²½ì œê´€ë¦¬ë°©ë²•','ë™ì¡±ëŒ€ê²°ì •ì±…','ì• êµ­ì£¼ì˜','ì• êµ­ì ì—´ì˜','í˜„ì‹ ']\n",
        "#ì¤‘ë³µ ì œê±°\n",
        "PoliticalTerms = list(set(PoliticalTerms))\n",
        "print(PoliticalTerms)\n",
        "#ê¸´ ë‹¨ì–´ë¶€í„° ì²˜ë¦¬\n",
        "PoliticalTerms.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtfpiCRMjyEe"
      },
      "source": [
        "## ë¶ˆìš©ì–´ ì‚¬ì „\n",
        "ì•„ë˜ ì‚¬ì´íŠ¸ì—ì„œ êµ¬í•œ ë¶ˆìš©ì–´ ì‚¬ì „ì„ í˜„ì¬ ì£¼ì–´ì§„ ë°ì´í„°ì˜ ìƒíƒœì— ë§ì¶”ì–´ ìˆ˜ì •í•˜ì˜€ë‹¤. ì´ë•Œ, í•œ ê¸€ìë¡œ ëœ ë‹¨ì–´ëŠ” ì˜¤ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ ë‘ ê¸€ì ì´ìƒì˜ ë¶ˆìš©ì–´ë§Œ ì‚¬ìš©í•˜ê² ë‹¤.\n",
        "\n",
        "ì°¸ê³ í•œ ì‚¬ì´íŠ¸: https://gist.github.com/spikeekips/40eea22ef4a89f629abd87eed535ac6a#file-stopwords-ko-txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zI13PUBJjx4Z"
      },
      "outputs": [],
      "source": [
        "stopwords = []\n",
        "with open('/Users/kimsuyeon/Desktop/Project2_TM/stopwords-ko.txt', 'r') as f:\n",
        "    list_file = f.readlines()\n",
        "\n",
        "for word in list_file:\n",
        "    stopword = word.split('\\n')[0]\n",
        "    if len(stopword) > 1:\n",
        "        stopwords.append(stopword)\n",
        "\n",
        "#ê¸´ ë‹¨ì–´ë¶€í„° ì²˜ë¦¬\n",
        "stopwords.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-19Qw9Skb6N"
      },
      "source": [
        "## ë™ì˜ì–´ ì‚¬ì „\n",
        "\n",
        "í•œ ê¸°ê´€ì— ëŒ€í•´ ë‚˜íƒ€ë‚´ëŠ” ìš©ì–´ê°€ ë§ì€ ê²ƒìœ¼ë¡œ í™•ì¸ ë˜ì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ì¡°ì„ ë¡œë™ë‹¹ì„ ì˜ë¯¸í•˜ëŠ” ë™ì˜ì–´ë¡œëŠ” 'ë‹¹','ë¡œë™ë‹¹'ì´ ìˆì—ˆë‹¤. ë¶í•œ ê¸°ê´€ì— ëŒ€í•´ ìì„¸í•˜ê²ŒëŠ” ëª¨ë¥´ë¯€ë¡œ ìµœëŒ€í•œ ì•„ëŠ” ì„ ì—ì„œ ë™ì˜ì–´ ì‚¬ì „ì„ ì •ì˜í•˜ê² ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1t_Uf7mXkgCf"
      },
      "outputs": [],
      "source": [
        "#ë™ì˜ì–´ ì‚¬ì „ ì •ì˜\n",
        "synonym_dict = {\n",
        "    'ë†ì´Œ': ['ë†ì´Œë°œì „ì „ëµ','ë†ì´Œê±´ì„¤ê°•ë ¹','ë†ì´Œ','ë†ì´Œë¬¸ì œí•´ê²°','ë†ì´Œê±´ì„¤','ë†ì´Œë§ˆì„','ë†ì´Œë°œì „','ë†ì´Œìƒí™œí™˜ê²½','ë†ì´Œìƒí™œ',\"ë†ì´Œë¬¸ì œ\",'ë†ì´Œê²½ë¦¬'],\n",
        "    'ë†ì—…': ['ë†ì—…ê·¼ë¡œì','ë†ì—…ë¶€','ë†ì—…ìƒì‚°','ë†ì—…','ë†ì—…ì§€ë„ì¼êµ°','ë°€ë†ì‚¬ê²½í—˜','ë°€ë†ì‚¬','ë†ì—…ë°œì „','ë†ì—…ì§€ë„','ë†ê¸°ê³„','ì•Œê³¡ê³ ì§€','ë†ì¥','ì‚´ë¦¼ì§‘','í˜‘ë™ë†ì¥'],\n",
        "    'í˜ëª…':['ì¡°ì„ í˜ëª…','í˜ëª…'],\n",
        "    'ì‚¬íšŒì£¼ì˜':['ì‚¬íšŒì£¼ì˜',\"ê³µí™”êµ­\"],\n",
        "    'ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­':['ì¡°êµ­','ì¡°ì„ ë¯¼ì£¼ì£¼ì˜ì¸ë¯¼ê³µí™”êµ­','êµ­ê°€','ë‚˜ë¼','ì¡°ì„ ','ë‹¹êµ­','ì¡°ì„ ë°˜ë„','ëŒ€ì¡°ì„ ','ìš°ë¦¬ë‚˜ë¼'],\n",
        "    'ê´´ë¢°': ['ê´´ë¢°íŒ¨ë‹¹','ê´´ë¢°êµ°ë¶€','ê´´ë¢°êµ°ë¬´ë ¥','ê´´ë¢°ì •ê¶Œ','ë‚¨ì¡°ì„ ê²ƒë“¤','ë°˜í†µì¼ì‚¬ëŒ€ë§¤êµ­ì„¸ë ¥','ë‚¨ì¡°ì„ í˜¸ì „ê´‘'],\n",
        "    'ëŒ€í•œë¯¼êµ­': ['ëŒ€í•œë¯¼êµ­','ë‚¨ì¡°ì„ ','ë‚¨ë…˜','ë‚¨ì¡°ì„ ë‹¹êµ­'],\n",
        "    'ê¹€ì •ì€': ['ìµœê³ ì§€ë„ì','ê¹€ì •ì€','ì´ë¹„ì„œ','ì¡°ì„ ë¡œë™ë‹¹ ìœ„ì›ì¥ë™ì§€'],\n",
        "    'ë¶ë‚¨': ['ë¶ë‚¨ê´€ê³„','ë¶ë‚¨','ë¶ê³¼ ë‚¨','ë¶ë‚¨ë‹¹êµ­'],\n",
        "    'ë¯¼ì£¼ì£¼ì˜': ['ë°˜ê³µí™”êµ­','ììœ ë¯¼ì£¼ì£¼ì˜','ë¯¼ì£¼ì£¼ì˜'],\n",
        "    'ë¯¸êµ­': ['ëŒ€ë¯¸','ëŒ€ë¯¸ëŒ€ì ','ë¯¸êµ­','ìœ ì—”êµ°ì‚¬ë ¹ë¶€','ì™¸ì„¸','ë¯¸ì œì¹¨ëµêµ°','ï¼–.ï¼‘ï¼’ì¡°ë¯¸ê³µë™ì„±ëª…','ì¡°ë¯¸ìˆ˜ë‡Œìƒë´‰', 'ì¡°ë¯¸','ë¯¸êµ°'],\n",
        "    'ìš°ë¦¬': ['ìš°ë¦¬ì‹', 'ìš°ë¦¬ ì‹','ìš°ë¦¬ë‹¹', 'ìš°ë¦¬ ë‹¹','ìš°ë¦¬ë¯¼ì¡±','ìš°ë¦¬ ë¯¼ì¡±','ìš°ë¦¬í˜','ìš°ë¦¬ í˜',\"ìš°ë¦¬ë‚˜ë¼\"],\n",
        "    'ì¡°ì„ ë¡œë™ë‹¹': ['ì¡°ì„ ë¡œë™ë‹¹', 'ë¡œë™ë‹¹','ë‹¹'],\n",
        "    'ë‹¹ì¤‘ì•™ìœ„ì›íšŒ': ['ë‹¹ ì¤‘ì•™ìœ„ì›íšŒ', 'ë‹¹ì¤‘ì•™ìœ„ì›íšŒ','ì¤‘ì•™ìœ„ì›íšŒ'],\n",
        "    'ë‹¹ì¤‘ì•™ê²€ì‚¬ìœ„ì›íšŒ':['ë‹¹ ì¤‘ì•™ê²€ì‚¬ìœ„ì›íšŒ','ë‹¹ì¤‘ì•™ê²€ì‚¬ìœ„ì›íšŒ','ì¤‘ì•™ê²€ì‚¬ìœ„ì›íšŒ'],\n",
        "    'ë‹¹ì¤‘ì•™êµ°ì‚¬ìœ„ì›íšŒ':['ë‹¹ ì¤‘ì•™êµ°ì‚¬ìœ„ì›íšŒ','ë‹¹ì¤‘ì•™êµ°ì‚¬ìœ„ì›íšŒ','ì¤‘ì•™êµ°ì‚¬ìœ„ì›íšŒ'],\n",
        "    'ë‹¹ì¤‘ì•™ìœ„ì›ì¥ì œ' : ['ë‹¹ ì¤‘ì•™ìœ„ì›ì¥ì œ','ë‹¹ì¤‘ì•™ìœ„ì›ì¥ì œ','ì¤‘ì•™ìœ„ì›ì¥ì œ'],\n",
        "    'ë‹¹ëŒ€í‘œìíšŒ':['ë‹¹ëŒ€í‘œìíšŒ',\"ë‹¹ ëŒ€í‘œìíšŒ\",'ë‹¹ ëŒ€íšŒ','ë‹¹ëŒ€íšŒ','ëŒ€íšŒ'],\n",
        "    'ê²½ì œ' : ['ê²½ì œê´€ë¦¬ë°©ë²•','ì¸ë¯¼ê²½ì œë°œì „','ë¯¼ì¡±ê²½ì œ','ê²½ì œê±´ì„¤','ê²½ì œë¶€','ê²½ì œë°œì „','ê²½ì œ','ì¸ë¯¼ê²½ì œ','ê²½ì œê°•êµ­ê±´ì„¤','ê±´ì„¤ê²½ì œë ¥', 'ê²½ì œì ', 'ê²½ì œì œì¬', 'ìë¦½ê²½ì œ','êµ­ê°€ê²½ì œ', 'ê²½ì œë°œì „','ê²½ì œì‚¬ì—…','ì‚¬íšŒê²½ì œ','ê²½ì œêµ¬ì¡°','ì§€ë°©ê²½ì œ','ê²½ì œí™œì„±í™”'],\n",
        "    'í•µ' : ['í•µ','í•µìœ„ê¸°ì‚¬íƒœ','ì¤‘í•µ','í•µë¬´ì¥ë ¥ í•µì‹œí—˜','í•µìœ„í˜‘','ë¹„í•µí™”','í•µë¬´ê¸°','í•µì „ìŸ','í•µë¬´ë ¥','í•µíƒ„ë‘','ì—´í•µë¬´ê¸°','í•µê°•êµ­','ìˆ˜ì†Œí­íƒ„','ëŒ€ë¥™ê°„ íƒ„ë„ë¡œì¼“','ìˆ˜ì†Œíƒ„','í•µë°˜ê²©','í•µì–µì œë ¥','í•µíƒ€ê²©','í•µíƒ„ë‘'],\n",
        "    'í†µì¼':['í†µì¼','ê³µë™ì„ ì–¸','ì¡°êµ­í†µì¼ë¡œì„ ','íŒë¬¸ì ì„ ì–¸','ï¼™ì›”í‰ì–‘ê³µë™ì„ ì–¸','ì¡°êµ­í†µì¼','í†µì¼ì ','ì„¸ê³„í‰í™”','í‰í™”ë²ˆì˜','ï¼–.ï¼‘ï¼•','ï¼‘ï¼.ï¼”ì„ ì–¸ë°œí‘œ','ï¼‘ï¼.ï¼”ì„ ì–¸ë°œí‘œ','ï¼‘ï¼.ï¼”','ï¼–.ï¼‘ï¼•ê³µë™ì„ ì–¸','6.15ê³µë™ì„ ì–¸','6.15ì„ ì–¸','10.4ì„ ì–¸','í‰í™”í†µì¼','í†µì¼ë°©ìš´','í‰í™”í˜‘ì •','ë‚¨ë¶í†µì¼','ìì£¼í†µì¼','í†µì¼ìš´ë™','í†µì¼ëŒ€íšŒ','í†µì¼ëŒ€ì§„êµ°','7Â·4ê³µë™ì„±ëª…','ì¡°êµ­í†µì¼3ëŒ€ì›ì¹™', '6Â·15ê³µë™ì„ ì–¸', '10Â·4ì„ ì–¸','í†µì¼í—Œì¥','í†µì¼ëŒ€ê°•'],\n",
        "    'êµ°ìˆ˜':['êµ°ìˆ˜','êµ°ìˆ˜ê³µì—…'],\n",
        "    'êµ­ë°©':['êµ­ë°©','êµ­ë°©ê³µì—…','í˜„ëŒ€ì ë¬´ì¥ì¥ë¹„','ë³‘ê¸°ì°½','êµ­ë°©ë ¥','êµ­ë°©ë ¥','êµ°ëŒ€',' êµ° ','ì¸ë¯¼êµ°','ì¸ë¯¼êµ°ëŒ€','êµ°ì‚¬','ì¥ë³‘','ì „ëµì „ìˆ ì ë°©ì¹¨','ì „ëµì „ìˆ ','ì „ìŸë„ë°œì±…ë™','êµ°ë ¥','í˜ëª…ì ë ¹êµ°ì²´ê³„','ì¸ë¯¼êµ°ëŒ€','ì¡°ì„ ì¸ë¯¼ë‚´ë¬´êµ°','ë‚´ë¬´êµ°', 'êµ°ê¸°','êµ°í’','ìµœì •ì˜ˆí˜ëª…ê°•êµ°','ë°±ë‘ì‚°í˜ëª…ê°•êµ°','ë°±ë‘ì‚°í›ˆë ¨','ì „íˆ¬ë™ì›íƒœì„¸','êµ°ì¸'],\n",
        "    'ê³¼í•™':['ê³¼í•™','ê³¼í•™í™”','ê³¼í•™ì','ê³¼í•™ì','ê³¼í•™ê¸°ìˆ ','ê³¼í•™êµìœ¡','ê³¼í•™ê¸°ìˆ ì„±ê³¼','ê³¼í•™ì—°êµ¬','ê³¼í•™ê¸°ìˆ ì¤‘ì‹œê¸°í’','ê³¼í•™ê¸°ìˆ ','ì „ë¯¼ê³¼í•™ê¸°ìˆ ì¸ì¬í™”','í˜„ëŒ€ê³¼í•™ê¸°ìˆ '],\n",
        "    'ìš°ì£¼':['ì‹¤ìš©ìœ„ì„±','ìš°ì£¼','ì§€êµ¬ê´€ì¸¡ìœ„ì„±','ìœ„ì„±','ê´‘ëª…ì„±-4','ì •ì§€ìœ„ì„±','ìš´ë°˜ë¡œì¼“ìš©','ì§€ìƒë¶„ì¶œì‹œí—˜','ìš°ì£¼ì •ë³µ','ê³¼í•™ê¸°ìˆ ìœ„ì„±','ì¸ê³µì§€êµ¬ìœ„ì„±'],\n",
        "    'ë ¥ì‚¬':['ë ¥ì‚¬ì ','ë ¥ì‚¬'],\n",
        "    'ì œêµ­ì£¼ì˜':['ì•„ì‹œì•„íƒœí‰ì–‘ì§€ë°°ì „ëµ','ë°˜ë™ì„¸ë ¥','ëŒ€ì•„ì‹œì•„ì§€ë°°ì „ëµ','ë°˜ê³µí™”êµ­ì „ìŸì±…ë™','ë¶ì¹¨ì „ìŸì†Œë™','ë°˜ê³µí™”êµ­ì œì¬','ë™ì¡±ëŒ€ê²°','ì‚¬ëŒ€ë§¤êµ­','ì‚¬ëŒ€ë§¤êµ­ì±…ë™'],\n",
        "    'ëŸ¬ì‹œì•„':['ë¡œì”¨ì•¼','ëŸ¬ì‹œì•„'],\n",
        "    'ê¹€ì •ì¼':['ì–´ë²„ì´','ê¹€ì •ì¼','ì–´ë²„ì´ìˆ˜ë ¹','ì–´ë²„ì´ì¥êµ°ë‹˜'],\n",
        "    'ì¹œì„ ':['ì¹œì„ í˜‘ì¡°ê´€ê³„','ì¹œì„ í˜‘ì¡°','ì¹œì„ '],\n",
        "    'ì• êµ­':['ì• êµ­ì‚¬ì—…','ì• êµ­í—Œì‹ ','ì• êµ­ì—´','ì• êµ­ë¯¸','ì• êµ­','ì• êµ­ì','ì• êµ­ì£¼ì˜','ì• êµ­ì ì—´ì˜','í˜„ì‹ '],\n",
        "    'ë™ì§€':['ë™í¬','ë™ì§€','ê²¨ë¡€'],\n",
        "    'ì˜ë„':['ë ¹ë„','ì˜ë„'],\n",
        "    'ê°•êµ­':['ì‚¬íšŒì£¼ì˜ê°•ì„±êµ­ê°€','ì²œí•˜ì œì¼ê°•êµ­','ì‚¬íšŒì£¼ì˜ê°•êµ­','ê²½ì œê°•êµ­','ê°•ì„±êµ­ê°€'],\n",
        "    'ì •ì¹˜ì‚¬ìƒ':['ì •ì¹˜ì‚¬ìƒì‚¬ì—…','ì •ì¹˜ì‚¬ìƒê°•êµ­','ì •ì¹˜ì‚¬ìƒ'],\n",
        "    'ë¬¸ëª…':['ë¬¸ëª…ê°œí™”ê¸°','ë¬¸í™”ì˜ˆìˆ ','ëª…ì‘','ê·¹ì¥','ì•¼ì™¸ê·¹ì¥','ì¶œíŒ','ë¬¸í™”í›„ìƒì‹œì„¤', 'ê³µì›','ìœ ì›ì§€'],\n",
        "    'ì—ë„¤ë¥´ê¸°':['í’ë ¥','ìˆ˜ë ¥','ì§€ì—´','íƒœì–‘ì—´','ì—ë„¤ë¥´ê¸°','ìì—°ì—ë„¤ë¥´ê¸°'],\n",
        "    'ìˆ˜ì‚°ì—…':['ì›ì–‘ì–´','ì–‘ì–´','ë°”ë‹¤','ìˆ˜ì‚°','ì–‘ì‹'],\n",
        "    'ì¼ê¾¼':['ê·¼ë¡œë‹¨ì²´ì¡°ì§','ì¼ê¾¼','ê·¼ë¡œì¸ë¯¼ëŒ€ì¤‘','ë¡œë†ì ìœ„êµ°']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAgV8RuKlBTN"
      },
      "source": [
        "## ë¶í•œ ìš©ì–´ì‚¬ì „ í™œìš©\n",
        "1) í•œ ê¸€ìë¡œ ëœ ë‹¨ì–´ëŠ” ì˜¤ë¶„ë¥˜ìœ¨ì„ ë†’ì´ë¯€ë¡œ ë‘ ê¸€ìë¡œ ëœ ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê² ë‹¤.\n",
        "\n",
        "2) ë™ì‚¬ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ '-ë‹¤'ë¡œ ëë‚˜ëŠ” ë‹¨ì–´ëŠ” ì‚­ì œí•˜ê² ë‹¤.\n",
        "\n",
        "3) ë¬¸ì¥ë„ í¬í•¨ë˜ì–´ ìˆì´ë¯€ë¡œ ê·¸ ë¶€ë¶„ì€ ì‚­ì œí•˜ê² ë‹¤.\n",
        "\n",
        "ì°¸ê³  ì‚¬ì´íŠ¸:https://nkinfo.unikorea.go.kr/nkp/word/nkword.do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "G03KIeLQlUCJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "NorthKoreaTerms = pd.read_csv(\"/Users/kimsuyeon/Desktop/Project2_TM/á„‡á…®á†¨á„’á…¡á†«á„‹á…­á†¼á„‹á…¥á„‰á…¡á„Œá…¥á†«.csv\")\n",
        "NorthKoreaTerms = [word for word in NorthKoreaTerms[38:]['ìš©ì–´'] if ' ' not in word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0O-wjd5JlErZ"
      },
      "outputs": [],
      "source": [
        "#2)ë™ì‚¬ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ '-ë‹¤'ë¡œ ëë‚˜ëŠ” ë‹¨ì–´ëŠ” ì‚­ì œí•˜ê² ë‹¤.\n",
        "#3)í•œ ê¸€ìì¸ ë‹¨ì–´ì™€ ëŒ€í•œì´ë¼ëŠ” ë‹¨ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.(ëŒ€í•œì˜ ê²½ìš° ì•ì„œ ì‚¬ì „ì—ì„œ ì •ì˜í•œ ëŒ€í•œë¯¼êµ­ì´ë¼ëŠ” ë‹¨ì–´ì™€ í˜¼ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)\n",
        "NKTerms = []\n",
        "for word in NorthKoreaTerms:\n",
        "    if ((word[-1] != 'ë‹¤') | (word != 'ëŒ€í•œ')) & (len(word) != 1):\n",
        "        NKTerms.append(word)\n",
        "\n",
        "#ê¸´ ë‹¨ì–´ë¶€í„° ì²˜ë¦¬\n",
        "NKTerms.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSrPB41ZkChq"
      },
      "source": [
        "# 2ï¸âƒ£í•„ìš”í•œ ì‹ ì •ì˜\n",
        "\n",
        "1) ì—°ë„ë³„ í…ìŠ¤íŠ¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì‹\n",
        "\n",
        "2) ì‚¬ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶ˆìš©ì–´(stopwords) ì œê±°í•˜ëŠ” ì‹\n",
        "\n",
        "3) ì‚¬ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ëª…ì‚¬ ì¶”ì¶œí•˜ëŠ” ì‹\n",
        "\n",
        "4) ë™ì˜ì–´ë¥¼ ë§¤í•‘í•˜ëŠ” ì‹\n",
        "\n",
        "5) text cleaning ì‹\n",
        ": í•œ ê¸€ìë¡œëœ ë‹¨ì–´, ìŠ¤í˜ì´ìŠ¤ ê³µê°„ì„ ëª¨ë‘ = ' 'ë¡œ ë³€ê²½\n",
        "\n",
        "6) ê²°ê³¼ ì—‘ì…€ë¡œ ë‚´ë³´ë‚´ëŠ” ì‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "AYc8LueKj6e4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#1)í…ìŠ¤íŠ¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì‹\n",
        "def read_data(sheet_name,header):\n",
        "    return pd.read_excel(\"/Users/kimsuyeon/Desktop/Project2_TM/á„‰á…µá†«á„‚á…§á†«á„‰á…¡_á„á…©á†¼á„’á…¡á†¸_0812.xlsx\",sheet_name=sheet_name,header=header)\n",
        "\n",
        "#2)ë¶ˆìš©ì–´ ì œê±° ì‹\n",
        "def remove_stopwords(text,stopwords_list):\n",
        "    remove_stopwords_text = text\n",
        "    for noun in stopwords_list:\n",
        "        if noun in remove_stopwords_text:\n",
        "                remove_stopwords_text = remove_stopwords_text.replace(noun, \"  \")\n",
        "    return remove_stopwords_text\n",
        "\n",
        "\n",
        "#3)ì‚¬ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ëª…ì‚¬ ì¶”ì¶œí•˜ëŠ” ì‹\n",
        "from collections import Counter\n",
        "def extract_nouns(text, noun_list):\n",
        "    extracted_nouns = []\n",
        "    remaining_text = text\n",
        "    for noun in noun_list:\n",
        "        if noun in remaining_text:\n",
        "            count = remaining_text.count(noun)\n",
        "            extracted_nouns.append((noun,count))\n",
        "            remaining_text = remaining_text.replace(noun, \" \")\n",
        "    return extracted_nouns, remaining_text\n",
        "\n",
        "#4)ë™ì˜ì–´ë¥¼ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜(ì •ì˜í•œ ì‚¬ì „ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜(soynlpì—ì„œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜X))\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "def synonym(word, synonym_dict):\n",
        "    for key, synonyms in synonym_dict.items():\n",
        "        if word in synonyms:\n",
        "            return key\n",
        "    return word\n",
        "\n",
        "#synonyms_freq: dataì•ˆì— ë™ì˜ì–´ ì²˜ë¦¬ë¥¼ í•˜ê³  ê°ê°ì˜ ë‹¨ì–´ì— ëŒ€í•œ ë¹ˆë„ìˆ˜ë¥¼ dictionary í˜•íƒœë¡œ ì¶œë ¥\n",
        "def synonyms_freq(data, synonym_dict):\n",
        "    \"\"\"\n",
        "    ë‹¨ì–´ì™€ ë¹ˆë„ ë°ì´í„°ë¥¼ ë™ì˜ì–´ ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ë³‘í•©\n",
        "    Parameters:\n",
        "        data (dict ë˜ëŠ” list): ë‹¨ì–´ì™€ ë¹ˆë„ ë°ì´í„°ë¥¼ í¬í•¨í•œ dict ë˜ëŠ” (ë‹¨ì–´, ë¹ˆë„) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸\n",
        "        synonym_dict (dict): ë™ì˜ì–´ ì‚¬ì „ìœ¼ë¡œ, keyëŠ” ëŒ€í‘œ ë‹¨ì–´, valueëŠ” ë™ì˜ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "    Returns:\n",
        "        dict: ë³‘í•©ëœ ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬\n",
        "    \"\"\"\n",
        "    #dataê°€ ë¦¬ìŠ¤íŠ¸ë¼ë©´ (ë‹¨ì–´, ë¹ˆë„) í˜•ì‹ìœ¼ë¡œ dictë¡œ ë³€í™˜\n",
        "    if isinstance(data, list):\n",
        "        word_frequencies = {}\n",
        "        for sublist in data:\n",
        "            for word, freq in sublist:\n",
        "                word_frequencies[word] = word_frequencies.get(word, 0) + freq\n",
        "    #dictë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "    elif isinstance(data, Counter):\n",
        "        word_frequencies = data.copy()\n",
        "\n",
        "    merged_frequencies = {}\n",
        "    #ë™ì˜ì–´ ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆë„ ë³‘í•©\n",
        "    for main_word, synonym_list in synonym_dict.items():\n",
        "        total_count = word_frequencies.get(main_word, 0)\n",
        "\n",
        "        for synonym in synonym_list:\n",
        "            total_count += word_frequencies.get(synonym, 0)\n",
        "            #ë™ì˜ì–´ëŠ” ë¹ˆë„ë¥¼ í•©ì‚°í•œ í›„ ì œê±°\n",
        "            if synonym in word_frequencies:\n",
        "                del word_frequencies[synonym]\n",
        "\n",
        "        if total_count > 0:\n",
        "            merged_frequencies[main_word] = total_count\n",
        "\n",
        "    #ë™ì˜ì–´ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë‹¨ì–´ ì¶”ê°€\n",
        "    for word, count in word_frequencies.items():\n",
        "        if word not in merged_frequencies:\n",
        "            merged_frequencies[word] = count\n",
        "\n",
        "    return merged_frequencies\n",
        "\n",
        "#5)text cleaning\n",
        "def clean_text(data):\n",
        "    remaining_text =[]\n",
        "    for sentence in data:\n",
        "        sentences = ''\n",
        "        for char in sentence.split(' '):\n",
        "            if len(char)>1:\n",
        "                sentences += char + ' '\n",
        "        remaining_text.append(sentences)\n",
        "    return remaining_text\n",
        "\n",
        "#6)ê²°ê³¼ ì—‘ì…€ë¡œ ë‚´ë³´ë‚´ëŠ” ì‹\n",
        "#ë‹¨ì–´-ë¹ˆë„ ê²°ê³¼ ì €ì¥ ë°ì´í„°ì…‹\n",
        "doc_word_freq = {}\n",
        "file_path = '/Users/kimsuyeon/Desktop/doc_word_freq.csv'\n",
        "\n",
        "def export_to_csv(doc_word_freq, file_path):\n",
        "    # ëª¨ë“  ë‹¨ì–´ë¥¼ ìˆ˜ì§‘\n",
        "    all_words = set()\n",
        "    for freq_dict in doc_word_freq.values():\n",
        "        all_words.update(freq_dict.keys())\n",
        "\n",
        "    all_words = sorted(all_words)\n",
        "\n",
        "    #DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "    data = []\n",
        "    for doc_name, freq_dict in doc_word_freq.items():\n",
        "        row = [freq_dict.get(word, 0) for word in all_words]\n",
        "        data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(data, index=doc_word_freq.keys(), columns=all_words)\n",
        "    df.to_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8EdYPA2brFc"
      },
      "source": [
        "# 3ï¸âƒ£ë‹¨ì–´ í† í°í™” ë° ë‹¨ì–´ ë¹ˆë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y1smDFVmZp5"
      },
      "source": [
        "## ğŸ“Œ2024ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3PqN6-cImgcx"
      },
      "outputs": [],
      "source": [
        "df_23end = read_data(0,1) #sheet1:2023.12.31\n",
        "all = df_23end['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "\n",
        "all_23end = []\n",
        "for text in all:\n",
        "    all_23end.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bo7h5Hqlm4gE"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#ì´ë¦„ ì¶”ì¶œ\n",
        "extracted_name = []\n",
        "for sen in all_23end:\n",
        "    if 'ë™ì§€' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if 'ë™ì§€' in char:\n",
        "                split_char = char.split('ë™ì§€')\n",
        "                if len(split_char) > 1:\n",
        "                    extracted_name.append(split_char[-2][-3:])\n",
        "#ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "name,texts_without_name  = [],[]\n",
        "#ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "for text in all_23end:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "    texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_23end = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgPMgwETqb1j"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "G6bgK99Iqcaz"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_23end = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnPZgKLg6CEh"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PcJ4QlxzscRA"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqYnb6D-7MJr"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPga5OcAsnkP"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_23end =Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNoY_aRm7xSe"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WtaJVGXaszu9"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_23end = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn-c0XWd7z7Z"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsKMwjFUBgAe"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_23end = Counter(mecab_nouns)\n",
        "mecab_23end\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X0ppxol715x"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRGJUhlutF2r"
      },
      "outputs": [],
      "source": [
        "word_all_23end = mecab_23end+soynlp_23end+name_23end+NKterms_23end+PoliticalTerms_23end\n",
        "combined_all_23end = synonyms_freq(word_all_23end, synonym_dict)\n",
        "\n",
        "combined_all_23end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyV_xctqtYXI"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RhpwXfB6tzV1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-C9VkytbZs"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J48g2mr5tRPW"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Joj0nTjntjVI"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XAVIibtnNu"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_oFEvcPtupl"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DsYmXS4ttan"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-GmzcN8FS5v"
      },
      "source": [
        "## ğŸ“Œ2023ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GrRqMBf4Fchn"
      },
      "outputs": [],
      "source": [
        "df_23be = read_data(1,2) #sheet2: 2023.01.01\n",
        "\n",
        "all = df_23be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n\\u3000\\u3000')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if '1. ' in sent or '2. 'in sent or '3. ' in sent or '4. ' in sent or '5. 'in sent or '6. 'in sent:\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_23be = []\n",
        "for text in all:\n",
        "    all_23be.append(re.sub(r'[\\n\\u3000]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uZxy-HDlF1kR"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#ì´ë¦„\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_23be:\n",
        "    if 'ë™ì§€' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if 'ë™ì§€' in char:\n",
        "                name = char.split('ë™ì§€')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "name,texts_without_name  = [],[]\n",
        "#ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "for text in all_23be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "    texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_23be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDyecVlzF1kS"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "X5CzmYFUF1kS"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_23be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxDMgyAoF1kT"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "TdfUMwnjF1kT"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU8wMcd4F1kT"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCX2ceW4F1kU"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_23be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYE8vBDyF1kV"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vZmvBbz2F1kV"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_23be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiO8RGoyF1kW"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7fho_5MF1kX"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_23be = Counter(mecab_nouns)\n",
        "mecab_23be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxm6mWKbF1kX"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzhZTVm3F1kX"
      },
      "outputs": [],
      "source": [
        "word_all_23be = mecab_23be+soynlp_23be+name_23be+NKterms_23be+PoliticalTerms_23be\n",
        "combined_all_23be = synonyms_freq(word_all_23be, synonym_dict)\n",
        "\n",
        "combined_all_23be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXad4mRwF1kY"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "s2EFRh0CF1kY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_2cx1hrF1kZ"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTKIKz9iF1kZ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk_ekaOFF1kZ"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHmXCIrXF1ka"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3ruAaSJF1ka"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP9h07vSF1ka"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z58SFXzaHble"
      },
      "source": [
        "## ğŸ“Œ2022ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5hjYwyr8Hblf"
      },
      "outputs": [],
      "source": [
        "df_22be = read_data(2,3) #sheet3: 2022.01.01\n",
        "\n",
        "all = df_22be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_22be = []\n",
        "for text in all:\n",
        "    all_22be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "zL5SclwvHblg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#ì´ë¦„\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_22be:\n",
        "    if 'ë™ì§€' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if 'ë™ì§€' in char:\n",
        "                name = char.split('ë™ì§€')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "name,texts_without_name  = [],[]\n",
        "#ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "for text in all_22be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "    texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_22be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YD30MSWHblg"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "yBLCl71DHblg"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_22be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gABu1rCRHblg"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "WI6cHASQHblh"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6aNdYZ-Hblh"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2XnDyMxHblh"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_22be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVVuUpKCHbli"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5HKraiTFHbli"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_22be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoEKyabjHblj"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb9y0CWTHblj"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_22be = Counter(mecab_nouns)\n",
        "mecab_22be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMgCX_yZHblj"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a9BxjhKHblj"
      },
      "outputs": [],
      "source": [
        "word_all_22be = mecab_22be+soynlp_22be+name_22be+NKterms_22be+PoliticalTerms_22be\n",
        "combined_all_22be = synonyms_freq(word_all_22be, synonym_dict)\n",
        "\n",
        "combined_all_22be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juGOGPVOHblk"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "EegAVFIMHblk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlsBwRvbHblk"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqBowy4CHbll"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrBcH76DHbll"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEIzlCrbHbll"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHj5h6G5Hbll"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABA31ixKHblm"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdYYiWT_JBkr"
      },
      "source": [
        "## ğŸ“Œ2021ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "iIgLcdhcJcZK"
      },
      "outputs": [],
      "source": [
        "df_21be = read_data(3,3) #sheet4: 2021.01.01\n",
        "\n",
        "all = df_21be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_21be = []\n",
        "for text in all:\n",
        "    all_21be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "F681wS4vJcZZ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#ì´ë¦„\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_21be:\n",
        "    if 'ë™ì§€' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if 'ë™ì§€' in char:\n",
        "                name = char.split('ë™ì§€')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "name,texts_without_name  = [],[]\n",
        "#ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "for text in all_21be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "    texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_21be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gqO7_ytJcZa"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "svJ5F1BTJcZa"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_21be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFs8bRrRJcZa"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "qdgLcW9YJcZa"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF7ZEOIKJcZb"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoqqkKlAJcZb"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_21be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwXXlCWpJcZc"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "cEDNoExMJcZc"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_21be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_LDqiGJcZc"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FgxIYdAJcZc"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_21be = Counter(mecab_nouns)\n",
        "mecab_21be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzeN_4T_JcZd"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "i0cLUmRhJcZd"
      },
      "outputs": [],
      "source": [
        "word_all_21be = mecab_21be+soynlp_21be+name_21be+NKterms_21be+PoliticalTerms_21be\n",
        "combined_all_21be = synonyms_freq(word_all_21be, synonym_dict)\n",
        "\n",
        "combined_all_21be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBAdl25EJcZd"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "VTP-xwcpJcZd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MbueJrQJcZe"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaQYkD7dJcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqFSX-IVJcZe"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7hqLrB1JcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21cKhCxVJcZe"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_wRZgnhJcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5-V4lNjJEDL"
      },
      "source": [
        "## ğŸ“Œ2020ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "sG9j2KFKKNjY"
      },
      "outputs": [],
      "source": [
        "df_20be = read_data(4,2)\n",
        "\n",
        "all = df_20be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_20be = []\n",
        "for text in all:\n",
        "    all_20be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "HzJVaOssKNjm"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#ì´ë¦„\n",
        "extracted_name = []\n",
        "for sen in all_20be:\n",
        "    if 'ë™ì§€' in sen :\n",
        "        word = sen.split(' ')\n",
        "        for idx,char in enumerate(word):\n",
        "            if 'ë™ì§€' in char:\n",
        "                name = char.split('ë™ì§€')\n",
        "                if name[0] == '':\n",
        "                    extracted_name.append(word[idx-1])\n",
        "                elif name[0] != 'ìœ„ì›ì¥':\n",
        "                    extracted_name.append(name[0])\n",
        "\n",
        "#ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "name,texts_without_name  = [],[]\n",
        "#ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "for text in all_20be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "    texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_20be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAro0AysKNjm"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "c2R1Yt80KNjm"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_20be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u01v1NMgKNjm"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "0lCLrH-pKNjm"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiVA_0NxKNjm"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z1UUzCgKNjn"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_20be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ArVYL36KNjn"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Z36_EuslKNjn"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_20be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTay7KfUKNjo"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI5Fb0utKNjo"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_20be = Counter(mecab_nouns)\n",
        "mecab_20be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rwmcdY-KNjo"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FqKwNW9KNjo"
      },
      "outputs": [],
      "source": [
        "word_all_20be = mecab_20be+soynlp_20be+name_20be+NKterms_20be+PoliticalTerms_20be\n",
        "combined_all_20be = synonyms_freq(word_all_20be, synonym_dict)\n",
        "\n",
        "combined_all_20be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K8DwGC6KNjo"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Lqoe5NGvKNjp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPhZxxREKNjp"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxoFUfMqKNjp"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3k8Ul5YKNjp"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPXMjJUhKNjp"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtdhFgA9KNjq"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM8Y15vHKNjq"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22tveb6HJGWW"
      },
      "source": [
        "## ğŸ“Œ2019ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "lYqrSQ9fM80c"
      },
      "outputs": [],
      "source": [
        "df_19be = read_data(5,3)\n",
        "\n",
        "all = df_19be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_19be = []\n",
        "for text in all:\n",
        "    all_19be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_19be = all_19be[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "-gazII3lM80o"
      },
      "outputs": [],
      "source": [
        "# ì „ë¬¸ì— ì´ë¦„ ì—†ë‹¤.\n",
        "# from collections import Counter\n",
        "\n",
        "# #ì´ë¦„\n",
        "# extracted_name = []\n",
        "# for sen in all_19be:\n",
        "#     if 'ë™ì§€' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if 'ë™ì§€' in char:\n",
        "#                 name = char.split('ë™ì§€')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != 'ìœ„ì›ì¥':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "# name,texts_without_name  = [],[]\n",
        "# #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "# for text in all_19be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_19be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVF1vFYjM80p"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "PNDzHn_2M80p"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_19be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_19be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQZlmH_M80p"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "kI5ZJARDM80p"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViwprXmxM80q"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw7h50CjM80q"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_19be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ5LDHDsM80q"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "LFD2YkYxM80r"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_19be = Counter(word_frequencies_NKterms) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jh0NYkJM80r"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAqyjCfUM80r"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_19be = Counter(mecab_nouns)\n",
        "mecab_19be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmBSlw9bM80r"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "8j8nUCOmM80s"
      },
      "outputs": [],
      "source": [
        "word_all_19be = mecab_19be+soynlp_19be+NKterms_19be+PoliticalTerms_19be\n",
        "combined_all_19be = synonyms_freq(word_all_19be, synonym_dict)\n",
        "\n",
        "combined_all_19be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-eVngsxM80s"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ZhKqiNleM80s"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9II_w4aM80t"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubnDMMI4M80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59InN9_WM80t"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQdAxaWdM80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNKb9CQgM80t"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4jlFsoYM80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkdS5p_JIBI"
      },
      "source": [
        "## ğŸ“Œ2018ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "k9NCsSV0NCXr"
      },
      "outputs": [],
      "source": [
        "df_18be = read_data(6,3)\n",
        "\n",
        "all = df_18be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_18be = []\n",
        "for text in all:\n",
        "    all_18be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_18be = all_18be[1:]\n",
        "all_18be[-1] = all_18be[-1][:60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "XJnkR24nNCXs"
      },
      "outputs": [],
      "source": [
        "#ì „ë¬¸ì— ì´ë¦„ ì—†ë‹¤.\n",
        "# from collections import Counter\n",
        "\n",
        "# #ì´ë¦„\n",
        "# extracted_name = []\n",
        "# for sen in all_18be:\n",
        "#     if 'ë™ì§€' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if 'ë™ì§€' in char:\n",
        "#                 name = char.split('ë™ì§€')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != 'ìœ„ì›ì¥':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "# name,texts_without_name  = [],[]\n",
        "# #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "# for text in all_18be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_18be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmhAMB7BNCXt"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "VlOMlCHKNCXt"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_18be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_18be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjhkbeb2NCXt"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "gR8HlipyNCXt"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMTEeGosNCXu"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQe3JgVlNCXu"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_18be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azMyo3QENCXv"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "ZUbVfzm1NCXv"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_18be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU0MYxjeNCXw"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCWj2CE7NCXw"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_18be = Counter(mecab_nouns)\n",
        "mecab_18be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDPtrpBcNCXw"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "zbdOj7gZNCXw"
      },
      "outputs": [],
      "source": [
        "word_all_18be = mecab_18be+soynlp_18be+NKterms_18be+PoliticalTerms_18be\n",
        "combined_all_18be = synonyms_freq(word_all_18be, synonym_dict)\n",
        "\n",
        "combined_all_18be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTrBeE1TNCXx"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "4iUJ8A6zNCXx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkR0ImEgNCXx"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3q_RAtHNCXx"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCBfHBwrNCXy"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mUm6kv2NCXy"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epWQ5PLiNCXy"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC3QYoLANCXy"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PPcav8cJJul"
      },
      "source": [
        "## ğŸ“Œ2017ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "XFd854PRNOLN"
      },
      "outputs": [],
      "source": [
        "df_17be = read_data(7,3)\n",
        "all = df_17be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_17be = []\n",
        "for text in all:\n",
        "    all_17be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "5nG4baiqNOLN"
      },
      "outputs": [],
      "source": [
        "#ì „ë¬¸ì— ì´ë¦„ ì—†ë‹¤.\n",
        "# from collections import Counter\n",
        "\n",
        "# #ì´ë¦„\n",
        "# extracted_name = []\n",
        "# for sen in all_17be:\n",
        "#     if 'ë™ì§€' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if 'ë™ì§€' in char:\n",
        "#                 name = char.split('ë™ì§€')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != 'ìœ„ì›ì¥':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "# name,texts_without_name  = [],[]\n",
        "# #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "# for text in all_17be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_17be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZAPxsK9NOLN"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "f1a5MvJ4NOLO"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_17be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_17be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kec6OZCWNOLO"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "vazu2cpKNOLO"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOwTLhnNOLO"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRVa4rmxNOLO"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_17be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v82QA13NOLO"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "q7BxfvX6NOLP"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_17be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N_aSo-fNOLP"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF0I7brMNOLP"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_17be = Counter(mecab_nouns)\n",
        "mecab_17be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGaqz7gHNOLP"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "pkusM_6QNOLP"
      },
      "outputs": [],
      "source": [
        "word_all_17be = mecab_17be+soynlp_17be+NKterms_17be+PoliticalTerms_17be\n",
        "combined_all_17be = synonyms_freq(word_all_17be, synonym_dict)\n",
        "\n",
        "combined_all_17be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PFdmCS5NOLP"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "-4Ot7VMkNOLQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6q0I-c-NOLQ"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS0qEncVNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAyg6tMBNOLQ"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISFIkHeVNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDhB2ul2NOLQ"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oer1DOyQNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6B6dTJLwB"
      },
      "source": [
        "## ğŸ“Œ2016ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "Miv3XqMYNMkv"
      },
      "outputs": [],
      "source": [
        "df_16be = read_data(8,3)\n",
        "\n",
        "all = df_16be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_16be = []\n",
        "for text in all:\n",
        "    all_16be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "Ju5zdLrINMk8"
      },
      "outputs": [],
      "source": [
        "# ì „ë¬¸ì— ì´ë¦„ ì—†ë‹¤.\n",
        "# from collections import Counter\n",
        "\n",
        "# #ì´ë¦„\n",
        "# extracted_name = []\n",
        "# for sen in all_16be:\n",
        "#     if 'ë™ì§€' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if 'ë™ì§€' in char:\n",
        "#                 name = char.split('ë™ì§€')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != 'ìœ„ì›ì¥':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "# name,texts_without_name  = [],[]\n",
        "# #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "# for text in all_16be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_16be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjzO2h84NMk8"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "rxa-cJ91NMk8"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_16be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_16be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMtQ27NMNMk8"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "fjfjznAMNMk9"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-AUBZEGNMk9"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9lg1_2WNMk9"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_16be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5k-7HWNMk9"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "n23d2TiCNMk9"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_16be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ9ls1BQNMk-"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc4AE65ONMk-"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_16be = Counter(mecab_nouns)\n",
        "mecab_16be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQk4lTDVNMk-"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "ajU76TIZNMk-"
      },
      "outputs": [],
      "source": [
        "word_all_16be = mecab_16be+soynlp_16be+NKterms_16be+PoliticalTerms_16be\n",
        "combined_all_16be = synonyms_freq(word_all_16be, synonym_dict)\n",
        "\n",
        "combined_all_16be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-_3IjhNMk_"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Ri1tmJJ4NMk_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wNh9Jv9NMk_"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDIye0EdNMk_"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vve8JzyFNMk_"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPAiVlUHNMk_"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9s4eDZ3NMlA"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9w_j_RQNMlA"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKdXB7B-JM6p"
      },
      "source": [
        "## ğŸ“Œ2015ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "eK-IpVQKNKxy"
      },
      "outputs": [],
      "source": [
        "df_15be = read_data(9,3)\n",
        "\n",
        "all = df_15be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_15be = []\n",
        "for text in all:\n",
        "    all_15be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_15be = all_15be[2:]\n",
        "all_15be = all_15be[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "MxwOSd3mNKxy"
      },
      "outputs": [],
      "source": [
        "#ì „ë¬¸ì— ì´ë¦„ ì–¸ê¸‰ ì—†ë‹¤.\n",
        "# from collections import Counter\n",
        "\n",
        "# #ì´ë¦„\n",
        "# extracted_name = []\n",
        "# for sen in all_15be:\n",
        "#     if 'ë™ì§€' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if 'ë™ì§€' in char:\n",
        "#                 name = char.split('ë™ì§€')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != 'ìœ„ì›ì¥':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "# name,texts_without_name  = [],[]\n",
        "# #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "# for text in all_15be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_15be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB-Em_9ENKxz"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "jkuDL1hjNKxz"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_15be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_15be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8jqhjfBNKxz"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "9xjKgH7oNKxz"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-whWJvNKxz"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF-GDXwbNKx0"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_15be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIB9V8ZsNKx0"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "jsQ75MKVNKx0"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_15be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOfatEjRNKx0"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a055JmQdNKx1"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_15be = Counter(mecab_nouns)\n",
        "mecab_15be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ywZH9AVNKx1"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "_qfTmPQuNKx1"
      },
      "outputs": [],
      "source": [
        "word_all_15be = mecab_15be+soynlp_15be+NKterms_15be+PoliticalTerms_15be\n",
        "combined_all_15be = synonyms_freq(word_all_15be, synonym_dict)\n",
        "\n",
        "combined_all_15be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQSGZKVRNKx2"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "AbipL2AtNKx2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsKmwR7TNKx2"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MlQIVPMNKx2"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bus9GfSGNKx2"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZqJERtLNKx3"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzbL8Rn4NKx3"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCMxcvyJNKx3"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVqTAB8JJOcj"
      },
      "source": [
        "## ğŸ“Œ2014ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "AeJ7M0IcNInN"
      },
      "outputs": [],
      "source": [
        "df_14be = read_data(10,2)\n",
        "\n",
        "all = df_14be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_14be = []\n",
        "for text in all:\n",
        "    all_14be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_14be[0] = all_14be[0].split(\".\")[2]\n",
        "all_14be = all_14be[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "y4q2fH2TNInY"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# #ì´ë¦„\n",
        "# extracted_name = []\n",
        "# for sen in all_14be:\n",
        "#     if 'ë™ì§€' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if 'ë™ì§€' in char:\n",
        "#                 name = char.split('ë™ì§€')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != 'ìœ„ì›ì¥':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "# name,texts_without_name  = [],[]\n",
        "# #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "# for text in all_14be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_14be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK_hPV4XNInY"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "qb8wiyuGNInZ"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_14be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_14be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQpSMbANInZ"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "b2JaJECINInZ"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIQlcwAWNInZ"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8OhcwxtNInZ"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_14be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQH-oCDhNIna"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "B-31J1MeNIna"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_14be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePV0uIMGNIna"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7tY-XlBNIna"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_14be = Counter(mecab_nouns)\n",
        "mecab_14be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biryHpFuNIna"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVY_O3jYNInb"
      },
      "outputs": [],
      "source": [
        "word_all_14be = mecab_14be+soynlp_14be+NKterms_14be+PoliticalTerms_14be\n",
        "combined_all_14be = synonyms_freq(word_all_14be, synonym_dict)\n",
        "\n",
        "combined_all_14be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fTaB7mNInb"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "zc7Ol5YpNInb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaeq3qi-NInb"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55gsTE_dNInb"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtcaOvm8NInb"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P42ocOVENInc"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we8fwgnDNInc"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvdL5rsvNInc"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvNUjt_cJQA_"
      },
      "source": [
        "## ğŸ“Œ2013ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "kqvPis-1NGO4"
      },
      "outputs": [],
      "source": [
        "df_13be = read_data(11,3)\n",
        "\n",
        "all = df_13be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_13be = []\n",
        "for text in all:\n",
        "    all_13be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "FLx657ExNGPI"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# #ì´ë¦„\n",
        "# extracted_name = []\n",
        "# for sen in all_13be:\n",
        "#     if 'ë™ì§€' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if 'ë™ì§€' in char:\n",
        "#                 name = char.split('ë™ì§€')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != 'ìœ„ì›ì¥':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "# name,texts_without_name  = [],[]\n",
        "# #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "# for text in all_13be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_13be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jegm7dJNGPI"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "Qv81aYQ-NGPJ"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_13be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_13be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyM7crpmNGPJ"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "Oj52IF2gNGPJ"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPEFpuluNGPJ"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlV7XoVINGPJ"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_13be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMXD5tdkNGPK"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "CIkRH71CNGPK"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_13be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AkmBHV2NGPK"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh0GWidJNGPL"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_13be = Counter(mecab_nouns)\n",
        "mecab_13be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPJ22egZNGPL"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdBZ1RhRNGPL"
      },
      "outputs": [],
      "source": [
        "word_all_13be = mecab_13be+soynlp_13be+NKterms_13be+PoliticalTerms_13be\n",
        "combined_all_13be = synonyms_freq(word_all_13be, synonym_dict)\n",
        "\n",
        "combined_all_13be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1m4XORqNGPM"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "QF7JUkqsNGPM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL-L8UONNGPM"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueVMRYicNGPM"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0taFNtbNGPN"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r1pB1cLNGPN"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj5gf40VNGPN"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP_f25AkNGPN"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TYJKz4YJRxv"
      },
      "source": [
        "## ğŸ“Œ2012ë…„ ì‹ ë…„ì‚¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "qkN-dmTgNDRU"
      },
      "outputs": [],
      "source": [
        "df_12be = read_data(12,3)\n",
        "\n",
        "all = df_12be['ì „ë¬¸'][0]\n",
        "\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì¼ì°¨ì ìœ¼ë¡œ ì œê±°\n",
        "import re\n",
        "all = re.sub(r'[()ã€Šã€‹!]', ' ', all)\n",
        "#'\\n\\n'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "all = all.split('\\n\\n')\n",
        "#íŠ¹ìˆ˜ê¸°í˜¸ ì´ì°¨ì ìœ¼ë¡œ ('1. ','2. ','3. ', '4. ' ,'\\n' ë“±) ì‚­ì œ\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#ì œëŒ€ë¡œ ì•ˆ ì§€ì›Œì§„ ê±° ë‹¤ì‹œ ì œê±°\n",
        "all_12be = []\n",
        "for text in all:\n",
        "    all_12be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "Nixtnz7LNDRV"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# #ì´ë¦„\n",
        "# extracted_name = []\n",
        "# for sen in all_12be:\n",
        "#     if 'ë™ì§€' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if 'ë™ì§€' in char:\n",
        "#                 name = char.split('ë™ì§€')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != 'ìœ„ì›ì¥':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #ì´ë¦„ ë¹ˆë„ êµ¬í•˜ê¸°\n",
        "# name,texts_without_name  = [],[]\n",
        "# #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ì•ì„œ ì¶”ì¶œëœ ì´ë¦„ ì œê±°\n",
        "# for text in all_12be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ extracted_nameì—ì„œ ì¶”ì¶œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ë©´ nameì— ì €ì¥\n",
        "#     texts_without_name.append(remaining_text) #extracted_nameì— ì¶”ì¶œëœ ì´ë¦„ì„ ì œê±°í•œ í…ìŠ¤íŠ¸\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_12be = Counter(word_frequencies_name) #ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHqu_erqNDRV"
      },
      "source": [
        "### ê¶Œë ¥êµ¬ì¡°ì™€ ì •ì¹˜í˜•íƒœ ê´€ë ¨ ë‹¨ì–´ ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ì„ ì‚¬ìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "cjN2lc7LNDRV"
      },
      "outputs": [],
      "source": [
        "#ëª…ì‚¬ ì¶”ì¶œ\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_12be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_12be = Counter(word_frequencies_PoliticalTerms)#ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6DZU8SZNDRV"
      },
      "source": [
        "### Stopwords ì œê±°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "Jymzkh9SNDRV"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"ì¡°ì„ ë¡œë™\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqqaxrXxNDRW"
      },
      "source": [
        "### soynlp ëª…ì‚¬ ì¶”ì¶œê¸° (ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTLmQf8tNDRW"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ ì¤‘ 2ê¸€ì ì´ìƒë§Œ ì €ì¥\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_12be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlpë¡œ ì¶”ì¶œëœ ëª…ì‚¬ë“¤ì„ texts_without_stopwordsì—ì„œ ì‚­ì œí•œë‹¤.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GDgX8r0NDRW"
      },
      "source": [
        "### ë¶í•œ ìš©ì–´ì‚¬ì „ì„ í™œìš©í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "ee9wuVy9NDRW"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#ë™ì˜ì–´ ì²˜ë¦¬\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_12be = Counter(word_frequencies_NKterms) # ë§ˆì§€ë§‰ì— ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì¹  ë•Œ, '+'ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´\n",
        "\n",
        "#' 'ì™€ ê°™ì´ ê³µë°± ì‚­ì œ\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHVX-NpNDRX"
      },
      "source": [
        "### Mecab ëª…ì‚¬ ì¶”ì¶œê¸°(ì´ë¯¸ ëª…ì‚¬ë¡œ ë¶„ë¥˜ëœ ë‹¨ì–´ë“¤ì€ ì œì™¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWPBx2O_NDRX"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #ë™ì˜ì–´ ì²˜ë¦¬\n",
        "\n",
        "#ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
        "mecab_12be = Counter(mecab_nouns)\n",
        "mecab_12be\n",
        "\n",
        "# í•„ìš”ì‹œ\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgD4NkxPNDRY"
      },
      "source": [
        "### ëª¨ë‘ í•©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuegRlNLNDRY"
      },
      "outputs": [],
      "source": [
        "word_all_12be = mecab_12be+soynlp_12be+NKterms_12be+PoliticalTerms_12be\n",
        "combined_all_12be = synonyms_freq(word_all_12be, synonym_dict)\n",
        "\n",
        "combined_all_12be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxUYclp0NDRZ"
      },
      "source": [
        "### ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "lBpVGOXKNDRZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnP4xTdHNDRa"
      },
      "source": [
        "#### ìƒìœ„ 10ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIsZ7HSZNDRa"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 10ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Guog4SyNDRa"
      },
      "source": [
        "#### ìƒìœ„ 20ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKAUIPJHNDRa"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 20ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIa2HW0uNDRb"
      },
      "source": [
        "#### ìƒìœ„ 30ê°œ ë‹¨ì–´(ì „ë¬¸)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSCrXH86NDRb"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"ìƒìœ„ 30ê°œì˜ ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.ylabel(\"ë¹ˆë„\", fontproperties=fontprop)\n",
        "plt.xlabel(\"ëª…ì‚¬\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8MFxw5SZwyn"
      },
      "source": [
        "## ê²°ê³¼ ì—‘ì…€ë¡œ ë‚´ë³´ë‚´ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_word_freq = {}\n",
        "doc_word_freq['23end'] = combined_all_23end\n",
        "doc_word_freq['23be'] = combined_all_23be\n",
        "doc_word_freq['22be'] = combined_all_22be\n",
        "doc_word_freq['21be'] = combined_all_21be\n",
        "doc_word_freq['20be'] = combined_all_20be\n",
        "doc_word_freq['19be'] = combined_all_19be\n",
        "doc_word_freq['18be'] = combined_all_18be\n",
        "doc_word_freq['17be'] = combined_all_17be\n",
        "doc_word_freq['16be'] = combined_all_16be\n",
        "doc_word_freq['15be'] = combined_all_15be\n",
        "doc_word_freq['14be'] = combined_all_14be\n",
        "doc_word_freq['13be'] = combined_all_13be\n",
        "doc_word_freq['12be'] = combined_all_12be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export_to_csv(doc_word_freq, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4ï¸âƒ£Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "from striprtf.striprtf import rtf_to_text\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "doc_term = pd.read_csv('/Users/kimsuyeon/Desktop/doc_word_freq.csv',index_col=0)\n",
        "#ë‹¨ì–´ ì¶”ì¶œ\n",
        "terms = list(doc_term.columns)\n",
        "\n",
        "#ë¶ˆìš©ì–´(stopwords) ì¶”ì¶œ\n",
        "with open('/Users/kimsuyeon/Desktop/remove_words.rtf', 'r', encoding='utf-8') as f:\n",
        "    rtf_content = f.read()\n",
        "remove_list = rtf_to_text(rtf_content)\n",
        "stopwords = set([word.strip() for word in remove_list.splitlines() if word.strip()])\n",
        "\n",
        "#ë¬¸ì„œë³„ ë‹¨ì–´ë¦¬ìŠ¤íŠ¸\n",
        "documents_words = []\n",
        "for doc in doc_term.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dictionary = corpora.Dictionary(documents_words)\n",
        "#ë¹ˆë„ í•„í„°ë§\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) # ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ\n",
        "#Bag-of-Words ìƒì„±\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ìµœì ì˜ í† í”½ ìˆ˜ ì°¾ê¸° (1)Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perplexity_values = []\n",
        "for i in range(2,8):\n",
        "  lda_model = LdaModel(corpus,num_topics=i,id2word=dictionary,random_state=42,passes=7,alpha=1.0,eta=0.5)\n",
        "  perplexity_values.append(lda_model.log_perplexity(corpus))\n",
        "\n",
        "plt.plot(range(2,8),perplexity_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"perplexity score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ìµœì ì˜ í† í”½ ìˆ˜ ì°¾ê¸° (2)coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coherence_values = []\n",
        "for i in range(2,8):\n",
        "  lda_model = LdaModel(corpus,num_topics=i,id2word=dictionary,random_state=42,passes=7,alpha=1.0,eta=0.5)\n",
        "  coherence_model_lda = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,topn=10)\n",
        "  coherence_lda = coherence_model_lda.get_coherence()\n",
        "  coherence_values.append(coherence_lda)\n",
        "\n",
        "plt.plot(range(2,8),coherence_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"coherence score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ìµœì ì˜ í† í”½ ìˆ˜:3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#ê° ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ì €ì¥\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = doc_term.index\n",
        "df.to_excel('doc_topic_distributions.xlsx', index=False)\n",
        "\n",
        "#ì‹œê°í™”\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df #1:êµ­ë°©,2:ì •ì¹˜,3:ì‚¬íšŒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì°¸ê³ (ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_range = range(2, 8)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = doc_term.index\n",
        "df.to_excel('doc_topic_distributions_best1_(1).xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_best1_(1).html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ì‹œê¸°ë³„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T1 = doc_term.loc[['12be', '13be', '14be', '15be', '16be']]  #2012-2016\n",
        "T2 = doc_term.loc[['17be', '18be', '19be']]                  #2017-2019\n",
        "covid_period = doc_term.loc[['20be', '21be']]                #2020-2021\n",
        "T3 = doc_term.loc[['22be', '23be','23end']]                  #2022-2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T1 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ìµœì ì˜ ì¡°í•© ì°¾ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ë¬¸ì„œë³„ ë‹¨ì–´ë¦¬ìŠ¤íŠ¸\n",
        "documents_words = []\n",
        "for doc in T1.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T1.index\n",
        "df.to_excel('doc_topic_distributions_T1.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T1.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ëª¨ë¸ë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T1.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#ê° ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ì €ì¥\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T1.index\n",
        "df.to_excel('doc_topic_distributions_T1.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T1.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T2 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ìµœì ì˜ ì¡°í•© ì°¾ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ë¬¸ì„œë³„ ë‹¨ì–´ë¦¬ìŠ¤íŠ¸\n",
        "documents_words = []\n",
        "for doc in T2.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T2.index\n",
        "df.to_excel('doc_topic_distributions_T2.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T2.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ëª¨ë¸ë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T2.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#ê° ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ì €ì¥\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T2.index\n",
        "df.to_excel('doc_topic_distributions_T2.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T2.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Covid Period (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ìµœì ì˜ ì¡°í•© ì°¾ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ë¬¸ì„œë³„ ë‹¨ì–´ë¦¬ìŠ¤íŠ¸\n",
        "documents_words = []\n",
        "for doc in covid_period.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) \n",
        "#ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ(í•´ë‹¹ ê¸°ê°„ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ ë§¤ìš° ì ìœ¼ë¯€ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = covid_period.index\n",
        "df.to_excel('doc_topic_distributions_covid_period.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_covid_period.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ëª¨ë¸ë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in covid_period.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.98) \n",
        "#ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ(í•´ë‹¹ ê¸°ê°„ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ ë§¤ìš° ì ìœ¼ë¯€ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9#,\n",
        "    #alpha=1.0,\n",
        "    #eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#ê° ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ì €ì¥\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = covid_period.index\n",
        "df.to_excel('doc_topic_distributions_covid_period.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_covid_period.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T3 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ìµœì ì˜ íŒŒë¼ë¯¸í„° ì¡°í•© ì°¾ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ë¬¸ì„œë³„ ë‹¨ì–´ë¦¬ìŠ¤íŠ¸\n",
        "documents_words = []\n",
        "for doc in T3.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T3.index\n",
        "df.to_excel('doc_topic_distributions_T3.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T3.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ëª¨ë¸ë§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T3.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #ë¶ˆìš©ì–´ ì œê±° + (ë‹¨ì–´,ë¹ˆë„ìˆ˜)ë¡œ êµ¬ì„±ëœ text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #ìµœì†Œ ë‘ ë²ˆ ì´ìƒ, ë¬¸ì„œì˜ 90%ë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¶”ì¶œ\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#ê° ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ì €ì¥\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T3.index\n",
        "df.to_excel('doc_topic_distributions_T3.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T3.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sSrPB41ZkChq",
        "7y1smDFVmZp5",
        "YyV_xctqtYXI",
        "D-GmzcN8FS5v",
        "Z58SFXzaHble",
        "juGOGPVOHblk",
        "KdYYiWT_JBkr",
        "bBAdl25EJcZd",
        "s5-V4lNjJEDL",
        "0K8DwGC6KNjo",
        "22tveb6HJGWW",
        "E-eVngsxM80s",
        "CSkdS5p_JIBI",
        "yTrBeE1TNCXx",
        "2PPcav8cJJul",
        "7PFdmCS5NOLP",
        "wjf6B6dTJLwB",
        "Zw-_3IjhNMk_",
        "hKdXB7B-JM6p",
        "TQSGZKVRNKx2",
        "xVqTAB8JJOcj",
        "-6fTaB7mNInb",
        "cvNUjt_cJQA_",
        "g1m4XORqNGPM",
        "0TYJKz4YJRxv",
        "O8MFxw5SZwyn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
