{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysrC0j6TjXHg"
      },
      "source": [
        "# 1️⃣사전 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ5A7FyJjWdr"
      },
      "source": [
        "## 권력구조와 정치형태 관련 단어 사용자 정의 사전\n",
        "명사 분류가 잘 안되는 근본적인 원인은 분류하려는 언어가 북한어이기 때문이다. 특히, 북한에는 권력구조와 정치형태를 나타내는 용어에는 한국과 다른점이 많다. 따라서, 북한정부포털에서 제공하는 권력구조와 정치형태에 대한 설명을 참고하여 사용자 정의 사전 리스트를 작성하겠다.\n",
        "\n",
        "참고한 사이트:https://nkinfo.unikorea.go.kr/nkp/pge/ps/jung.do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0IMcb9ljiXz"
      },
      "outputs": [],
      "source": [
        "#사전 정의\n",
        "PoliticalTerms = ['사회주의','조선노동당','수령','장군','우리식','우리 식','우리당','우리 당','우리민족','우리 민족','우리 힘','우리힘',\"우리나라\",'우리','로동당','조국','건설경제력', '경제적', '경제제재', '자립경제','국가경제',\n",
        "                      '조직지도부','선전선동부','간부부','경공업부','경제부','과학교육부','국제부','군수공업부','군정지도부','규율조사부','근로단체부','농업부','당역사연구소','조선로동당',\n",
        "                      '문서정리실','문화예술부','민방위부','법무부','재정경리부','총무부','10국','39호실','경제정책실','경제발전','경제사업','사회경제','경제구조','지방경제','경제활성화',\n",
        "                      '최고인민회의','국무위원회','내각','사법검찰기관','중앙재판소', '중앙검찰소', '인민정권','전원회의','령도','조선민주주의인민공화국','조선인민군','수령','리익',\n",
        "                      '혁명사상','로동계급','근로인민대중','핵심부대','전위부대','수령체제','인민대중','근로대중','중앙집권제','상의하달','조직생활','사상생활','조직지도부','선전선동부',\n",
        "                      '유일지배','이념','당무','향도자','최고지도자','전권','조선로동당대회','당 대회','당대회','당 대표자회','당대표자회','최고참모부','참모부'\n",
        "                      '의사결정','노선','정책',\"전략전술\",'통일','정치국','정치노선', '조직노선','조선로동당 위원장동지','화학','전력','교육','건설','나라','６.１５',\n",
        "                      '중앙지도기관','당중앙지도기관','중앙기관','당중앙기관','중앙검사위원회','당 중앙검사위원회','당중앙검사위원회','당중앙위원회','당 중앙위원회','중앙위원회',\n",
        "                      '상무위원','최고수위','위원장','총비서','비서','당대표자회',\"당 대표자회\",'대표자회', '당규약','인민경제발전', '정세', '과업','국방','경제','병진정책','경제건설','월남문제',\n",
        "                      '당 중앙위원장제','당중앙위원장제','중앙위원장제','직제','수령','제1비서직','중앙군사위원회','당중앙군사위원회','당 중앙군사위원회','최고지도기관','대행', '후보위원',\n",
        "                      '내외',\"논의\",'의결','권력기구','안건','확대회의','정무국','비서제','군사노선',\"공화국\",'무력','지휘','군수공업','국방사업','국방력','현대화','조직비서',\n",
        "                      '내각총리','주권기관','헌납운동','조선소년단','헌납','투쟁','계획','국방과학발전','국제정세','국위','국익','북남관계','대미대적','반공화국','대미','핵',\n",
        "                      '미국','북남','대적사업','조선반도','선박공업','국방공업','민방위무력','함선공업','무인항공공업','무인무장장비','무인','정찰위성','우주과학','미싸일','무장장비','대사변',\n",
        "                      '핵무기생산계획','남조선','핵위기사태','안보','무기체계개발계획','군사','인민군대','9.19북남군사분야합의','로골화','유엔군사령부','사회주의강성국가건설','인민군장병','조국통일위업','인민경제'\n",
        "                      '괴뢰패당','괴뢰군부','괴뢰군무력','괴뢰정권','망동','내각부총리','침략전쟁기도','군사분계선지역','농업위원회','대한민국','국가예산','예산','대회','윤석열',\n",
        "                      '조국통일로선','대북','통일','흡수통일','문재인','자유민주주의','민주주의','화성-18형','신형','고체연료','엔진','중거리','탄도미사일','중대과업','당 지방조직','지방조직',\n",
        "                      '집권체제','조선혁명','혁명','당 위원회','상하','위계','여타','기관','사회단체','지배력','행사','도,시,군','성,도,시,군','도,시,군인민회','도 당 위원회', '시 당 위원회', '군 당 위원회', '초급당', '분초급당', '부문당', '당원','최하','기층조직',\n",
        "                      '당세포','관할지역','중앙당','하부','조직체계','시 도 당위원회','도,시','시,군','국가','인민','시 군 당위원회', '초급 당위원회', '당 세포조직','세포조직', '집행위원회','비서처', ' 정 ', ' 군 ',' 도 ',' 시 ',' 성 ',\n",
        "                      '정권기관','입법','집행','인사','직책','보임','겸직' ,'행정','지위,''군사노선', '토의','전반','군대','정치위원','인민군','총정치국','집행부서','외곽단체','사회주의애국청년동맹','조선직업총동맹',\n",
        "                      '조선농업근로자동맹','조선사회주의여성동맹','사상교양','전위대','영도','국무위원회','최고주권기관', '최고인민회의', '내각','중앙검찰소','중앙재판소','사법기관','국무위원장',\n",
        "                      '국무위원회','정령', '총리','부총리', '위원장','임명','해임','최고영도자','총사령관','특사권','국가방위위원회','국방위원회','추대','재추대','입법권','최고주권기관',\n",
        "                      '정기회의', \"임시회의\",'상임위원회','대의원','법령','대내외' ,'제의','제1부위원장', '위원','선출','위원장', '부위원장', '서기장',\"직위자\",'국가예산','심의','승인','조약','비준',\n",
        "                      '폐기권','거수가결','예산위원회', \"법제위원회\", '외교위원회','부문위원회','정책안','법안','휴회','상임위원회', '보충안','심의', '국회','국제의회기구','신임장',\"소환장\",'상무회의',\n",
        "                      '국가관리','국방','집행기관','관리기관','정무원','국가주석','검찰기관','검찰','검찰소','특별검찰소','하급검찰소','상급검찰소','중앙검찰소장','인민재판소','특별재판소','인민참심원',\n",
        "                      '배심원','최고재판기관','중핵','핵무장력 핵시험','핵위협','비핵화','핵무기','핵전쟁','핵무력','열핵무기','핵강국','핵반격','핵억제력','핵타격','핵탄두','조국통일','통일적','평화통일','통일방운',\n",
        "                      '남북통일','자주통일','통일운동','통일대회','통일대진군','남조선','남조선것들','리념','리상','경제부','농업부','최고회의','백두산','해외동포','애국헌신','력사적','력사','전투', '돌파전','지구관측위성','광명성-4','정지위성','운반로켓용','지상분출시험',\n",
        "                      '우주정복','과학연구','노농적위군','2·8비날론연합기업소','전민총돌격전','박근혜','아시아태평양지배전략','제국주의' ,'반동세력','북과 남','국방분야','과학교육','과학기술성과',\n",
        "                      '수소폭탄','수소탄','대륙간 탄도로켓','국방력','핵탄두','7·4공동성명','조국통일3대원칙','6.15공동선언','6.15공동','10.4선언',\"6·15공동선언\",'１０.４선언발표','１０.４선언발표','１０.４','６.１５공동선언', '10·4선언','통일헌장','통일대강','전당초급당위원장대회','초급당조직','아시아태평양지배전략','대아시아지배전략','반공화국전쟁책동',\n",
        "                      '남조선당국','북남당국','남조선호전광','청년동맹','정전협정','평화협정','어머니당','세계평화','로씨야','강성대국건설','령도','영도','어버이','김정은','어버이수령','어버이장군님','직맹', '농근맹', '여맹 조직','선린우호','친선협조관계','친선협조','통일대회합',\n",
        "                      '사대매국','동족대결','군인','사대매국책동','세계평화','평화번영','전성기','중국','미국','외세','미제침략군','백두산영웅청년발전소','청천강계단식발전소','과학기술전당','미래과학자거리','장천남새전문협동농장','사상관철전',\n",
        "                      '당정책옹위전','경제강국건설','협동농장','로농적위군','인공지구위성','반공화국제재','북침전쟁소동','위성과학자주책지구', '김책공업종합대학','연풍과학자휴양소','오중흡7연대 칭호쟁취운동', '근위부대운동','정치사상사업','정치사상강국','애국사업',\n",
        "                      '애국헌신','동포','동지','김일성-김정일주의자','김일성','김정일','조국통일','장병','국방력','군대',' 군 ','조국해방전쟁승리기념관', '은하과학자거리', '문수물놀이장', '마식령스키장',\"전략전술적방침\",\n",
        "                      '６.１２조미공동성명','조미수뇌상봉', '조미','판문점선언','９월평양공동선언','석탄','문명개화기','겨례','남녘','대조선','농기계','알곡고지','농장','과학기술중시기풍','현대과학기술','과학기술','전민과학기술인재화','문화예술','명작','공동선언','풍력','수력','지열','태양열','에네르기','자연에네르기',\n",
        "                      '현대적무장장비','현대과학기술','과학기술위성','전쟁도발책동','사회주의강성국가','천하제일강국','사회주의강국','경제강국','경제건설','실용위성','우주','민족경제','과학화','강성국가','군력','혁명적령군체계','인민군대','조선인민내무군','내무군', '군기','군풍','최정예혁명강군','백두산혁명강군','백두산훈련','전투동원태세','군인','문화후생시설', '공원','유원지',\n",
        "                      '근로단체조직','일꾼','근로인민대중','정치사상','경제관리방법','동족대결정책','애국주의','애국적열의','현신']\n",
        "#중복 제거\n",
        "PoliticalTerms = list(set(PoliticalTerms))\n",
        "print(PoliticalTerms)\n",
        "#긴 단어부터 처리\n",
        "PoliticalTerms.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtfpiCRMjyEe"
      },
      "source": [
        "## 불용어 사전\n",
        "아래 사이트에서 구한 불용어 사전을 현재 주어진 데이터의 상태에 맞추어 수정하였다. 이때, 한 글자로 된 단어는 오분류될 가능성이 있으므로 두 글자 이상의 불용어만 사용하겠다.\n",
        "\n",
        "참고한 사이트: https://gist.github.com/spikeekips/40eea22ef4a89f629abd87eed535ac6a#file-stopwords-ko-txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zI13PUBJjx4Z"
      },
      "outputs": [],
      "source": [
        "stopwords = []\n",
        "with open('/Users/kimsuyeon/Desktop/Project2_TM/stopwords-ko.txt', 'r') as f:\n",
        "    list_file = f.readlines()\n",
        "\n",
        "for word in list_file:\n",
        "    stopword = word.split('\\n')[0]\n",
        "    if len(stopword) > 1:\n",
        "        stopwords.append(stopword)\n",
        "\n",
        "#긴 단어부터 처리\n",
        "stopwords.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-19Qw9Skb6N"
      },
      "source": [
        "## 동의어 사전\n",
        "\n",
        "한 기관에 대해 나타내는 용어가 많은 것으로 확인 되었다. 예를 들면, 조선로동당을 의미하는 동의어로는 '당','로동당'이 있었다. 북한 기관에 대해 자세하게는 모르므로 최대한 아는 선에서 동의어 사전을 정의하겠다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1t_Uf7mXkgCf"
      },
      "outputs": [],
      "source": [
        "#동의어 사전 정의\n",
        "synonym_dict = {\n",
        "    '농촌': ['농촌발전전략','농촌건설강령','농촌','농촌문제해결','농촌건설','농촌마을','농촌발전','농촌생활환경','농촌생활',\"농촌문제\",'농촌경리'],\n",
        "    '농업': ['농업근로자','농업부','농업생산','농업','농업지도일군','밀농사경험','밀농사','농업발전','농업지도','농기계','알곡고지','농장','살림집','협동농장'],\n",
        "    '혁명':['조선혁명','혁명'],\n",
        "    '사회주의':['사회주의',\"공화국\"],\n",
        "    '조선민주주의인민공화국':['조국','조선민주주의인민공화국','국가','나라','조선','당국','조선반도','대조선','우리나라'],\n",
        "    '괴뢰': ['괴뢰패당','괴뢰군부','괴뢰군무력','괴뢰정권','남조선것들','반통일사대매국세력','남조선호전광'],\n",
        "    '대한민국': ['대한민국','남조선','남녘','남조선당국'],\n",
        "    '김정은': ['최고지도자','김정은','총비서','조선로동당 위원장동지'],\n",
        "    '북남': ['북남관계','북남','북과 남','북남당국'],\n",
        "    '민주주의': ['반공화국','자유민주주의','민주주의'],\n",
        "    '미국': ['대미','대미대적','미국','유엔군사령부','외세','미제침략군','６.１２조미공동성명','조미수뇌상봉', '조미','미군'],\n",
        "    '우리': ['우리식', '우리 식','우리당', '우리 당','우리민족','우리 민족','우리힘','우리 힘',\"우리나라\"],\n",
        "    '조선로동당': ['조선로동당', '로동당','당'],\n",
        "    '당중앙위원회': ['당 중앙위원회', '당중앙위원회','중앙위원회'],\n",
        "    '당중앙검사위원회':['당 중앙검사위원회','당중앙검사위원회','중앙검사위원회'],\n",
        "    '당중앙군사위원회':['당 중앙군사위원회','당중앙군사위원회','중앙군사위원회'],\n",
        "    '당중앙위원장제' : ['당 중앙위원장제','당중앙위원장제','중앙위원장제'],\n",
        "    '당대표자회':['당대표자회',\"당 대표자회\",'당 대회','당대회','대회'],\n",
        "    '경제' : ['경제관리방법','인민경제발전','민족경제','경제건설','경제부','경제발전','경제','인민경제','경제강국건설','건설경제력', '경제적', '경제제재', '자립경제','국가경제', '경제발전','경제사업','사회경제','경제구조','지방경제','경제활성화'],\n",
        "    '핵' : ['핵','핵위기사태','중핵','핵무장력 핵시험','핵위협','비핵화','핵무기','핵전쟁','핵무력','핵탄두','열핵무기','핵강국','수소폭탄','대륙간 탄도로켓','수소탄','핵반격','핵억제력','핵타격','핵탄두'],\n",
        "    '통일':['통일','공동선언','조국통일로선','판문점선언','９월평양공동선언','조국통일','통일적','세계평화','평화번영','６.１５','１０.４선언발표','１０.４선언발표','１０.４','６.１５공동선언','6.15공동선언','6.15선언','10.4선언','평화통일','통일방운','평화협정','남북통일','자주통일','통일운동','통일대회','통일대진군','7·4공동성명','조국통일3대원칙', '6·15공동선언', '10·4선언','통일헌장','통일대강'],\n",
        "    '군수':['군수','군수공업'],\n",
        "    '국방':['국방','국방공업','현대적무장장비','병기창','국방력','국방력','군대',' 군 ','인민군','인민군대','군사','장병','전략전술적방침','전략전술','전쟁도발책동','군력','혁명적령군체계','인민군대','조선인민내무군','내무군', '군기','군풍','최정예혁명강군','백두산혁명강군','백두산훈련','전투동원태세','군인'],\n",
        "    '과학':['과학','과학화','과학자','과학자','과학기술','과학교육','과학기술성과','과학연구','과학기술중시기풍','과학기술','전민과학기술인재화','현대과학기술'],\n",
        "    '우주':['실용위성','우주','지구관측위성','위성','광명성-4','정지위성','운반로켓용','지상분출시험','우주정복','과학기술위성','인공지구위성'],\n",
        "    '력사':['력사적','력사'],\n",
        "    '제국주의':['아시아태평양지배전략','반동세력','대아시아지배전략','반공화국전쟁책동','북침전쟁소동','반공화국제재','동족대결','사대매국','사대매국책동'],\n",
        "    '러시아':['로씨야','러시아'],\n",
        "    '김정일':['어버이','김정일','어버이수령','어버이장군님'],\n",
        "    '친선':['친선협조관계','친선협조','친선'],\n",
        "    '애국':['애국사업','애국헌신','애국열','애국미','애국','애국자','애국주의','애국적열의','현신'],\n",
        "    '동지':['동포','동지','겨례'],\n",
        "    '영도':['령도','영도'],\n",
        "    '강국':['사회주의강성국가','천하제일강국','사회주의강국','경제강국','강성국가'],\n",
        "    '정치사상':['정치사상사업','정치사상강국','정치사상'],\n",
        "    '문명':['문명개화기','문화예술','명작','극장','야외극장','출판','문화후생시설', '공원','유원지'],\n",
        "    '에네르기':['풍력','수력','지열','태양열','에네르기','자연에네르기'],\n",
        "    '수산업':['원양어','양어','바다','수산','양식'],\n",
        "    '일꾼':['근로단체조직','일꾼','근로인민대중','로농적위군']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAgV8RuKlBTN"
      },
      "source": [
        "## 북한 용어사전 활용\n",
        "1) 한 글자로 된 단어는 오분류율을 높이므로 두 글자로 된 단어만 사용하겠다.\n",
        "\n",
        "2) 동사도 포함되어 있으므로 '-다'로 끝나는 단어는 삭제하겠다.\n",
        "\n",
        "3) 문장도 포함되어 있이므로 그 부분은 삭제하겠다.\n",
        "\n",
        "참고 사이트:https://nkinfo.unikorea.go.kr/nkp/word/nkword.do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "G03KIeLQlUCJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "NorthKoreaTerms = pd.read_csv(\"/Users/kimsuyeon/Desktop/Project2_TM/북한용어사전.csv\")\n",
        "NorthKoreaTerms = [word for word in NorthKoreaTerms[38:]['용어'] if ' ' not in word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0O-wjd5JlErZ"
      },
      "outputs": [],
      "source": [
        "#2)동사도 포함되어 있으므로 '-다'로 끝나는 단어는 삭제하겠다.\n",
        "#3)한 글자인 단어와 대한이라는 단어는 사용하지 않는다.(대한의 경우 앞서 사전에서 정의한 대한민국이라는 단어와 혼동할 수 있으므로)\n",
        "NKTerms = []\n",
        "for word in NorthKoreaTerms:\n",
        "    if ((word[-1] != '다') | (word != '대한')) & (len(word) != 1):\n",
        "        NKTerms.append(word)\n",
        "\n",
        "#긴 단어부터 처리\n",
        "NKTerms.sort(key=lambda x: -len(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSrPB41ZkChq"
      },
      "source": [
        "# 2️⃣필요한 식 정의\n",
        "\n",
        "1) 연도별 텍스트 데이터 불러오는 식\n",
        "\n",
        "2) 사전을 기준으로 불용어(stopwords) 제거하는 식\n",
        "\n",
        "3) 사전을 기준으로 명사 추출하는 식\n",
        "\n",
        "4) 동의어를 매핑하는 식\n",
        "\n",
        "5) text cleaning 식\n",
        ": 한 글자로된 단어, 스페이스 공간을 모두 = ' '로 변경\n",
        "\n",
        "6) 결과 엑셀로 내보내는 식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "AYc8LueKj6e4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#1)텍스트 데이터 불러오는 식\n",
        "def read_data(sheet_name,header):\n",
        "    return pd.read_excel(\"/Users/kimsuyeon/Desktop/Project2_TM/신년사_통합_0812.xlsx\",sheet_name=sheet_name,header=header)\n",
        "\n",
        "#2)불용어 제거 식\n",
        "def remove_stopwords(text,stopwords_list):\n",
        "    remove_stopwords_text = text\n",
        "    for noun in stopwords_list:\n",
        "        if noun in remove_stopwords_text:\n",
        "                remove_stopwords_text = remove_stopwords_text.replace(noun, \"  \")\n",
        "    return remove_stopwords_text\n",
        "\n",
        "\n",
        "#3)사전을 기준으로 명사 추출하는 식\n",
        "from collections import Counter\n",
        "def extract_nouns(text, noun_list):\n",
        "    extracted_nouns = []\n",
        "    remaining_text = text\n",
        "    for noun in noun_list:\n",
        "        if noun in remaining_text:\n",
        "            count = remaining_text.count(noun)\n",
        "            extracted_nouns.append((noun,count))\n",
        "            remaining_text = remaining_text.replace(noun, \" \")\n",
        "    return extracted_nouns, remaining_text\n",
        "\n",
        "#4)동의어를 매핑하는 함수(정의한 사전에서 사용하는 함수(soynlp에서 사용하는 함수X))\n",
        "#동의어 처리\n",
        "def synonym(word, synonym_dict):\n",
        "    for key, synonyms in synonym_dict.items():\n",
        "        if word in synonyms:\n",
        "            return key\n",
        "    return word\n",
        "\n",
        "#synonyms_freq: data안에 동의어 처리를 하고 각각의 단어에 대한 빈도수를 dictionary 형태로 출력\n",
        "def synonyms_freq(data, synonym_dict):\n",
        "    \"\"\"\n",
        "    단어와 빈도 데이터를 동의어 사전을 기반으로 병합\n",
        "    Parameters:\n",
        "        data (dict 또는 list): 단어와 빈도 데이터를 포함한 dict 또는 (단어, 빈도) 튜플의 리스트\n",
        "        synonym_dict (dict): 동의어 사전으로, key는 대표 단어, value는 동의어 리스트\n",
        "    Returns:\n",
        "        dict: 병합된 단어와 빈도수를 포함한 딕셔너리\n",
        "    \"\"\"\n",
        "    #data가 리스트라면 (단어, 빈도) 형식으로 dict로 변환\n",
        "    if isinstance(data, list):\n",
        "        word_frequencies = {}\n",
        "        for sublist in data:\n",
        "            for word, freq in sublist:\n",
        "                word_frequencies[word] = word_frequencies.get(word, 0) + freq\n",
        "    #dict라면 그대로 사용\n",
        "    elif isinstance(data, Counter):\n",
        "        word_frequencies = data.copy()\n",
        "\n",
        "    merged_frequencies = {}\n",
        "    #동의어 사전을 기반으로 빈도 병합\n",
        "    for main_word, synonym_list in synonym_dict.items():\n",
        "        total_count = word_frequencies.get(main_word, 0)\n",
        "\n",
        "        for synonym in synonym_list:\n",
        "            total_count += word_frequencies.get(synonym, 0)\n",
        "            #동의어는 빈도를 합산한 후 제거\n",
        "            if synonym in word_frequencies:\n",
        "                del word_frequencies[synonym]\n",
        "\n",
        "        if total_count > 0:\n",
        "            merged_frequencies[main_word] = total_count\n",
        "\n",
        "    #동의어로 처리되지 않은 단어 추가\n",
        "    for word, count in word_frequencies.items():\n",
        "        if word not in merged_frequencies:\n",
        "            merged_frequencies[word] = count\n",
        "\n",
        "    return merged_frequencies\n",
        "\n",
        "#5)text cleaning\n",
        "def clean_text(data):\n",
        "    remaining_text =[]\n",
        "    for sentence in data:\n",
        "        sentences = ''\n",
        "        for char in sentence.split(' '):\n",
        "            if len(char)>1:\n",
        "                sentences += char + ' '\n",
        "        remaining_text.append(sentences)\n",
        "    return remaining_text\n",
        "\n",
        "#6)결과 엑셀로 내보내는 식\n",
        "#단어-빈도 결과 저장 데이터셋\n",
        "doc_word_freq = {}\n",
        "file_path = '/Users/kimsuyeon/Desktop/doc_word_freq.csv'\n",
        "\n",
        "def export_to_csv(doc_word_freq, file_path):\n",
        "    # 모든 단어를 수집\n",
        "    all_words = set()\n",
        "    for freq_dict in doc_word_freq.values():\n",
        "        all_words.update(freq_dict.keys())\n",
        "\n",
        "    all_words = sorted(all_words)\n",
        "\n",
        "    #DataFrame으로 변환\n",
        "    data = []\n",
        "    for doc_name, freq_dict in doc_word_freq.items():\n",
        "        row = [freq_dict.get(word, 0) for word in all_words]\n",
        "        data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(data, index=doc_word_freq.keys(), columns=all_words)\n",
        "    df.to_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8EdYPA2brFc"
      },
      "source": [
        "# 3️⃣단어 토큰화 및 단어 빈도를 기준으로 시각화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y1smDFVmZp5"
      },
      "source": [
        "## 📌2024년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3PqN6-cImgcx"
      },
      "outputs": [],
      "source": [
        "df_23end = read_data(0,1) #sheet1:2023.12.31\n",
        "all = df_23end['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "\n",
        "all_23end = []\n",
        "for text in all:\n",
        "    all_23end.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bo7h5Hqlm4gE"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#이름 추출\n",
        "extracted_name = []\n",
        "for sen in all_23end:\n",
        "    if '동지' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if '동지' in char:\n",
        "                split_char = char.split('동지')\n",
        "                if len(split_char) > 1:\n",
        "                    extracted_name.append(split_char[-2][-3:])\n",
        "#이름 빈도 구하기\n",
        "name,texts_without_name  = [],[]\n",
        "#기존 텍스트에서 앞서 추출된 이름 제거\n",
        "for text in all_23end:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "    texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_23end = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgPMgwETqb1j"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "G6bgK99Iqcaz"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_23end = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnPZgKLg6CEh"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PcJ4QlxzscRA"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqYnb6D-7MJr"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPga5OcAsnkP"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_23end =Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNoY_aRm7xSe"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WtaJVGXaszu9"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_23end = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn-c0XWd7z7Z"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsKMwjFUBgAe"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_23end = Counter(mecab_nouns)\n",
        "mecab_23end\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X0ppxol715x"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRGJUhlutF2r"
      },
      "outputs": [],
      "source": [
        "word_all_23end = mecab_23end+soynlp_23end+name_23end+NKterms_23end+PoliticalTerms_23end\n",
        "combined_all_23end = synonyms_freq(word_all_23end, synonym_dict)\n",
        "\n",
        "combined_all_23end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyV_xctqtYXI"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RhpwXfB6tzV1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-C9VkytbZs"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J48g2mr5tRPW"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Joj0nTjntjVI"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XAVIibtnNu"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_oFEvcPtupl"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DsYmXS4ttan"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23end).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-GmzcN8FS5v"
      },
      "source": [
        "## 📌2023년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GrRqMBf4Fchn"
      },
      "outputs": [],
      "source": [
        "df_23be = read_data(1,2) #sheet2: 2023.01.01\n",
        "\n",
        "all = df_23be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n\\u3000\\u3000')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if '1. ' in sent or '2. 'in sent or '3. ' in sent or '4. ' in sent or '5. 'in sent or '6. 'in sent:\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_23be = []\n",
        "for text in all:\n",
        "    all_23be.append(re.sub(r'[\\n\\u3000]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uZxy-HDlF1kR"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#이름\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_23be:\n",
        "    if '동지' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if '동지' in char:\n",
        "                name = char.split('동지')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#이름 빈도 구하기\n",
        "name,texts_without_name  = [],[]\n",
        "#기존 텍스트에서 앞서 추출된 이름 제거\n",
        "for text in all_23be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "    texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_23be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDyecVlzF1kS"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "X5CzmYFUF1kS"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_23be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxDMgyAoF1kT"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "TdfUMwnjF1kT"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU8wMcd4F1kT"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCX2ceW4F1kU"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_23be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYE8vBDyF1kV"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vZmvBbz2F1kV"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_23be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiO8RGoyF1kW"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7fho_5MF1kX"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_23be = Counter(mecab_nouns)\n",
        "mecab_23be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxm6mWKbF1kX"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzhZTVm3F1kX"
      },
      "outputs": [],
      "source": [
        "word_all_23be = mecab_23be+soynlp_23be+name_23be+NKterms_23be+PoliticalTerms_23be\n",
        "combined_all_23be = synonyms_freq(word_all_23be, synonym_dict)\n",
        "\n",
        "combined_all_23be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXad4mRwF1kY"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "s2EFRh0CF1kY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_2cx1hrF1kZ"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTKIKz9iF1kZ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk_ekaOFF1kZ"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHmXCIrXF1ka"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3ruAaSJF1ka"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP9h07vSF1ka"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_23be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z58SFXzaHble"
      },
      "source": [
        "## 📌2022년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5hjYwyr8Hblf"
      },
      "outputs": [],
      "source": [
        "df_22be = read_data(2,3) #sheet3: 2022.01.01\n",
        "\n",
        "all = df_22be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_22be = []\n",
        "for text in all:\n",
        "    all_22be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "zL5SclwvHblg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#이름\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_22be:\n",
        "    if '동지' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if '동지' in char:\n",
        "                name = char.split('동지')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#이름 빈도 구하기\n",
        "name,texts_without_name  = [],[]\n",
        "#기존 텍스트에서 앞서 추출된 이름 제거\n",
        "for text in all_22be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "    texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_22be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YD30MSWHblg"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "yBLCl71DHblg"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_22be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gABu1rCRHblg"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "WI6cHASQHblh"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6aNdYZ-Hblh"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2XnDyMxHblh"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_22be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVVuUpKCHbli"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5HKraiTFHbli"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_22be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoEKyabjHblj"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb9y0CWTHblj"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_22be = Counter(mecab_nouns)\n",
        "mecab_22be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMgCX_yZHblj"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a9BxjhKHblj"
      },
      "outputs": [],
      "source": [
        "word_all_22be = mecab_22be+soynlp_22be+name_22be+NKterms_22be+PoliticalTerms_22be\n",
        "combined_all_22be = synonyms_freq(word_all_22be, synonym_dict)\n",
        "\n",
        "combined_all_22be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juGOGPVOHblk"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "EegAVFIMHblk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlsBwRvbHblk"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqBowy4CHbll"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrBcH76DHbll"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEIzlCrbHbll"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHj5h6G5Hbll"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABA31ixKHblm"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_22be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdYYiWT_JBkr"
      },
      "source": [
        "## 📌2021년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "iIgLcdhcJcZK"
      },
      "outputs": [],
      "source": [
        "df_21be = read_data(3,3) #sheet4: 2021.01.01\n",
        "\n",
        "all = df_21be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_21be = []\n",
        "for text in all:\n",
        "    all_21be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "F681wS4vJcZZ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#이름\n",
        "extracted_name = []\n",
        "\n",
        "for sen in all_21be:\n",
        "    if '동지' in sen:\n",
        "        words = sen.split(' ')\n",
        "        for char in words:\n",
        "            if '동지' in char:\n",
        "                name = char.split('동지')[0]\n",
        "                extracted_name.append(name[-3:])\n",
        "\n",
        "#이름 빈도 구하기\n",
        "name,texts_without_name  = [],[]\n",
        "#기존 텍스트에서 앞서 추출된 이름 제거\n",
        "for text in all_21be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "    texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_21be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gqO7_ytJcZa"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "svJ5F1BTJcZa"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_21be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFs8bRrRJcZa"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "qdgLcW9YJcZa"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF7ZEOIKJcZb"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoqqkKlAJcZb"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_21be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwXXlCWpJcZc"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "cEDNoExMJcZc"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_21be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_LDqiGJcZc"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FgxIYdAJcZc"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_21be = Counter(mecab_nouns)\n",
        "mecab_21be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzeN_4T_JcZd"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "i0cLUmRhJcZd"
      },
      "outputs": [],
      "source": [
        "word_all_21be = mecab_21be+soynlp_21be+name_21be+NKterms_21be+PoliticalTerms_21be\n",
        "combined_all_21be = synonyms_freq(word_all_21be, synonym_dict)\n",
        "\n",
        "combined_all_21be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBAdl25EJcZd"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "VTP-xwcpJcZd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MbueJrQJcZe"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaQYkD7dJcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqFSX-IVJcZe"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7hqLrB1JcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21cKhCxVJcZe"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_wRZgnhJcZe"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_21be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5-V4lNjJEDL"
      },
      "source": [
        "## 📌2020년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "sG9j2KFKKNjY"
      },
      "outputs": [],
      "source": [
        "df_20be = read_data(4,2)\n",
        "\n",
        "all = df_20be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_20be = []\n",
        "for text in all:\n",
        "    all_20be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "HzJVaOssKNjm"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#이름\n",
        "extracted_name = []\n",
        "for sen in all_20be:\n",
        "    if '동지' in sen :\n",
        "        word = sen.split(' ')\n",
        "        for idx,char in enumerate(word):\n",
        "            if '동지' in char:\n",
        "                name = char.split('동지')\n",
        "                if name[0] == '':\n",
        "                    extracted_name.append(word[idx-1])\n",
        "                elif name[0] != '위원장':\n",
        "                    extracted_name.append(name[0])\n",
        "\n",
        "#이름 빈도 구하기\n",
        "name,texts_without_name  = [],[]\n",
        "#기존 텍스트에서 앞서 추출된 이름 제거\n",
        "for text in all_20be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "    if extracted_nouns:\n",
        "      name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "    texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "name_20be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAro0AysKNjm"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "c2R1Yt80KNjm"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in texts_without_name:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_20be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u01v1NMgKNjm"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "0lCLrH-pKNjm"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiVA_0NxKNjm"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z1UUzCgKNjn"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_20be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ArVYL36KNjn"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Z36_EuslKNjn"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_20be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTay7KfUKNjo"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI5Fb0utKNjo"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_20be = Counter(mecab_nouns)\n",
        "mecab_20be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rwmcdY-KNjo"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FqKwNW9KNjo"
      },
      "outputs": [],
      "source": [
        "word_all_20be = mecab_20be+soynlp_20be+name_20be+NKterms_20be+PoliticalTerms_20be\n",
        "combined_all_20be = synonyms_freq(word_all_20be, synonym_dict)\n",
        "\n",
        "combined_all_20be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K8DwGC6KNjo"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Lqoe5NGvKNjp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPhZxxREKNjp"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxoFUfMqKNjp"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3k8Ul5YKNjp"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPXMjJUhKNjp"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtdhFgA9KNjq"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM8Y15vHKNjq"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_20be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22tveb6HJGWW"
      },
      "source": [
        "## 📌2019년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "lYqrSQ9fM80c"
      },
      "outputs": [],
      "source": [
        "df_19be = read_data(5,3)\n",
        "\n",
        "all = df_19be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_19be = []\n",
        "for text in all:\n",
        "    all_19be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_19be = all_19be[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "-gazII3lM80o"
      },
      "outputs": [],
      "source": [
        "# 전문에 이름 없다.\n",
        "# from collections import Counter\n",
        "\n",
        "# #이름\n",
        "# extracted_name = []\n",
        "# for sen in all_19be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #이름 빈도 구하기\n",
        "# name,texts_without_name  = [],[]\n",
        "# #기존 텍스트에서 앞서 추출된 이름 제거\n",
        "# for text in all_19be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "#     texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_19be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVF1vFYjM80p"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "PNDzHn_2M80p"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_19be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_19be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQZlmH_M80p"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "kI5ZJARDM80p"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViwprXmxM80q"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw7h50CjM80q"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_19be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ5LDHDsM80q"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "LFD2YkYxM80r"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_19be = Counter(word_frequencies_NKterms) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jh0NYkJM80r"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAqyjCfUM80r"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_19be = Counter(mecab_nouns)\n",
        "mecab_19be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmBSlw9bM80r"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "8j8nUCOmM80s"
      },
      "outputs": [],
      "source": [
        "word_all_19be = mecab_19be+soynlp_19be+NKterms_19be+PoliticalTerms_19be\n",
        "combined_all_19be = synonyms_freq(word_all_19be, synonym_dict)\n",
        "\n",
        "combined_all_19be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-eVngsxM80s"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ZhKqiNleM80s"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9II_w4aM80t"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubnDMMI4M80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59InN9_WM80t"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQdAxaWdM80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNKb9CQgM80t"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4jlFsoYM80t"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_19be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkdS5p_JIBI"
      },
      "source": [
        "## 📌2018년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "k9NCsSV0NCXr"
      },
      "outputs": [],
      "source": [
        "df_18be = read_data(6,3)\n",
        "\n",
        "all = df_18be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_18be = []\n",
        "for text in all:\n",
        "    all_18be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_18be = all_18be[1:]\n",
        "all_18be[-1] = all_18be[-1][:60]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "XJnkR24nNCXs"
      },
      "outputs": [],
      "source": [
        "#전문에 이름 없다.\n",
        "# from collections import Counter\n",
        "\n",
        "# #이름\n",
        "# extracted_name = []\n",
        "# for sen in all_18be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #이름 빈도 구하기\n",
        "# name,texts_without_name  = [],[]\n",
        "# #기존 텍스트에서 앞서 추출된 이름 제거\n",
        "# for text in all_18be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "#     texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_18be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmhAMB7BNCXt"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "VlOMlCHKNCXt"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_18be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_18be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjhkbeb2NCXt"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "gR8HlipyNCXt"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMTEeGosNCXu"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQe3JgVlNCXu"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_18be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azMyo3QENCXv"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "ZUbVfzm1NCXv"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_18be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU0MYxjeNCXw"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCWj2CE7NCXw"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_18be = Counter(mecab_nouns)\n",
        "mecab_18be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDPtrpBcNCXw"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "zbdOj7gZNCXw"
      },
      "outputs": [],
      "source": [
        "word_all_18be = mecab_18be+soynlp_18be+NKterms_18be+PoliticalTerms_18be\n",
        "combined_all_18be = synonyms_freq(word_all_18be, synonym_dict)\n",
        "\n",
        "combined_all_18be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTrBeE1TNCXx"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "4iUJ8A6zNCXx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkR0ImEgNCXx"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3q_RAtHNCXx"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCBfHBwrNCXy"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mUm6kv2NCXy"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epWQ5PLiNCXy"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC3QYoLANCXy"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_18be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PPcav8cJJul"
      },
      "source": [
        "## 📌2017년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "XFd854PRNOLN"
      },
      "outputs": [],
      "source": [
        "df_17be = read_data(7,3)\n",
        "all = df_17be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_17be = []\n",
        "for text in all:\n",
        "    all_17be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "5nG4baiqNOLN"
      },
      "outputs": [],
      "source": [
        "#전문에 이름 없다.\n",
        "# from collections import Counter\n",
        "\n",
        "# #이름\n",
        "# extracted_name = []\n",
        "# for sen in all_17be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #이름 빈도 구하기\n",
        "# name,texts_without_name  = [],[]\n",
        "# #기존 텍스트에서 앞서 추출된 이름 제거\n",
        "# for text in all_17be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "#     texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_17be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZAPxsK9NOLN"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "f1a5MvJ4NOLO"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_17be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_17be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kec6OZCWNOLO"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "vazu2cpKNOLO"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjOwTLhnNOLO"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRVa4rmxNOLO"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_17be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v82QA13NOLO"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "q7BxfvX6NOLP"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_17be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N_aSo-fNOLP"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF0I7brMNOLP"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_17be = Counter(mecab_nouns)\n",
        "mecab_17be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGaqz7gHNOLP"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "pkusM_6QNOLP"
      },
      "outputs": [],
      "source": [
        "word_all_17be = mecab_17be+soynlp_17be+NKterms_17be+PoliticalTerms_17be\n",
        "combined_all_17be = synonyms_freq(word_all_17be, synonym_dict)\n",
        "\n",
        "combined_all_17be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PFdmCS5NOLP"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "-4Ot7VMkNOLQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6q0I-c-NOLQ"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS0qEncVNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAyg6tMBNOLQ"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISFIkHeVNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDhB2ul2NOLQ"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oer1DOyQNOLQ"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_17be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6B6dTJLwB"
      },
      "source": [
        "## 📌2016년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "Miv3XqMYNMkv"
      },
      "outputs": [],
      "source": [
        "df_16be = read_data(8,3)\n",
        "\n",
        "all = df_16be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_16be = []\n",
        "for text in all:\n",
        "    all_16be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "Ju5zdLrINMk8"
      },
      "outputs": [],
      "source": [
        "# 전문에 이름 없다.\n",
        "# from collections import Counter\n",
        "\n",
        "# #이름\n",
        "# extracted_name = []\n",
        "# for sen in all_16be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #이름 빈도 구하기\n",
        "# name,texts_without_name  = [],[]\n",
        "# #기존 텍스트에서 앞서 추출된 이름 제거\n",
        "# for text in all_16be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "#     texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_16be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjzO2h84NMk8"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "rxa-cJ91NMk8"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_16be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_16be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMtQ27NMNMk8"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "fjfjznAMNMk9"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-AUBZEGNMk9"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9lg1_2WNMk9"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_16be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5k-7HWNMk9"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "n23d2TiCNMk9"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_16be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ9ls1BQNMk-"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc4AE65ONMk-"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_16be = Counter(mecab_nouns)\n",
        "mecab_16be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQk4lTDVNMk-"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "ajU76TIZNMk-"
      },
      "outputs": [],
      "source": [
        "word_all_16be = mecab_16be+soynlp_16be+NKterms_16be+PoliticalTerms_16be\n",
        "combined_all_16be = synonyms_freq(word_all_16be, synonym_dict)\n",
        "\n",
        "combined_all_16be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-_3IjhNMk_"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Ri1tmJJ4NMk_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wNh9Jv9NMk_"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDIye0EdNMk_"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vve8JzyFNMk_"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPAiVlUHNMk_"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9s4eDZ3NMlA"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9w_j_RQNMlA"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_16be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKdXB7B-JM6p"
      },
      "source": [
        "## 📌2015년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "eK-IpVQKNKxy"
      },
      "outputs": [],
      "source": [
        "df_15be = read_data(9,3)\n",
        "\n",
        "all = df_15be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_15be = []\n",
        "for text in all:\n",
        "    all_15be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_15be = all_15be[2:]\n",
        "all_15be = all_15be[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "MxwOSd3mNKxy"
      },
      "outputs": [],
      "source": [
        "#전문에 이름 언급 없다.\n",
        "# from collections import Counter\n",
        "\n",
        "# #이름\n",
        "# extracted_name = []\n",
        "# for sen in all_15be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #이름 빈도 구하기\n",
        "# name,texts_without_name  = [],[]\n",
        "# #기존 텍스트에서 앞서 추출된 이름 제거\n",
        "# for text in all_15be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "#     texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_15be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB-Em_9ENKxz"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "jkuDL1hjNKxz"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_15be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_15be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8jqhjfBNKxz"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "9xjKgH7oNKxz"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-whWJvNKxz"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF-GDXwbNKx0"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_15be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIB9V8ZsNKx0"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "jsQ75MKVNKx0"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_15be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOfatEjRNKx0"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a055JmQdNKx1"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_15be = Counter(mecab_nouns)\n",
        "mecab_15be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ywZH9AVNKx1"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "_qfTmPQuNKx1"
      },
      "outputs": [],
      "source": [
        "word_all_15be = mecab_15be+soynlp_15be+NKterms_15be+PoliticalTerms_15be\n",
        "combined_all_15be = synonyms_freq(word_all_15be, synonym_dict)\n",
        "\n",
        "combined_all_15be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQSGZKVRNKx2"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "AbipL2AtNKx2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsKmwR7TNKx2"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MlQIVPMNKx2"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bus9GfSGNKx2"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZqJERtLNKx3"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzbL8Rn4NKx3"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCMxcvyJNKx3"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_15be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVqTAB8JJOcj"
      },
      "source": [
        "## 📌2014년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "AeJ7M0IcNInN"
      },
      "outputs": [],
      "source": [
        "df_14be = read_data(10,2)\n",
        "\n",
        "all = df_14be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_14be = []\n",
        "for text in all:\n",
        "    all_14be.append(re.sub(r'[\\n]', ' ', text))\n",
        "all_14be[0] = all_14be[0].split(\".\")[2]\n",
        "all_14be = all_14be[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "y4q2fH2TNInY"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# #이름\n",
        "# extracted_name = []\n",
        "# for sen in all_14be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #이름 빈도 구하기\n",
        "# name,texts_without_name  = [],[]\n",
        "# #기존 텍스트에서 앞서 추출된 이름 제거\n",
        "# for text in all_14be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "#     texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_14be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK_hPV4XNInY"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "qb8wiyuGNInZ"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_14be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_14be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQpSMbANInZ"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "b2JaJECINInZ"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIQlcwAWNInZ"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8OhcwxtNInZ"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_14be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQH-oCDhNIna"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "B-31J1MeNIna"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_14be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePV0uIMGNIna"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7tY-XlBNIna"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_14be = Counter(mecab_nouns)\n",
        "mecab_14be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biryHpFuNIna"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVY_O3jYNInb"
      },
      "outputs": [],
      "source": [
        "word_all_14be = mecab_14be+soynlp_14be+NKterms_14be+PoliticalTerms_14be\n",
        "combined_all_14be = synonyms_freq(word_all_14be, synonym_dict)\n",
        "\n",
        "combined_all_14be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fTaB7mNInb"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "zc7Ol5YpNInb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaeq3qi-NInb"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55gsTE_dNInb"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtcaOvm8NInb"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P42ocOVENInc"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we8fwgnDNInc"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvdL5rsvNInc"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_14be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvNUjt_cJQA_"
      },
      "source": [
        "## 📌2013년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "kqvPis-1NGO4"
      },
      "outputs": [],
      "source": [
        "df_13be = read_data(11,3)\n",
        "\n",
        "all = df_13be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_13be = []\n",
        "for text in all:\n",
        "    all_13be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "FLx657ExNGPI"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# #이름\n",
        "# extracted_name = []\n",
        "# for sen in all_13be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #이름 빈도 구하기\n",
        "# name,texts_without_name  = [],[]\n",
        "# #기존 텍스트에서 앞서 추출된 이름 제거\n",
        "# for text in all_13be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "#     texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_13be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jegm7dJNGPI"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "Qv81aYQ-NGPJ"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_13be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_13be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyM7crpmNGPJ"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "Oj52IF2gNGPJ"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPEFpuluNGPJ"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlV7XoVINGPJ"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_13be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMXD5tdkNGPK"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "CIkRH71CNGPK"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_13be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AkmBHV2NGPK"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh0GWidJNGPL"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_13be = Counter(mecab_nouns)\n",
        "mecab_13be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPJ22egZNGPL"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdBZ1RhRNGPL"
      },
      "outputs": [],
      "source": [
        "word_all_13be = mecab_13be+soynlp_13be+NKterms_13be+PoliticalTerms_13be\n",
        "combined_all_13be = synonyms_freq(word_all_13be, synonym_dict)\n",
        "\n",
        "combined_all_13be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1m4XORqNGPM"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "QF7JUkqsNGPM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL-L8UONNGPM"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueVMRYicNGPM"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0taFNtbNGPN"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r1pB1cLNGPN"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj5gf40VNGPN"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP_f25AkNGPN"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_13be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TYJKz4YJRxv"
      },
      "source": [
        "## 📌2012년 신년사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "qkN-dmTgNDRU"
      },
      "outputs": [],
      "source": [
        "df_12be = read_data(12,3)\n",
        "\n",
        "all = df_12be['전문'][0]\n",
        "\n",
        "#특수기호 일차적으로 제거\n",
        "import re\n",
        "all = re.sub(r'[()《》!]', ' ', all)\n",
        "#'\\n\\n'를 기준으로 문장 나누기\n",
        "all = all.split('\\n\\n')\n",
        "#특수기호 이차적으로 ('1. ','2. ','3. ', '4. ' ,'\\n' 등) 삭제\n",
        "for idx,sent in enumerate(all):\n",
        "    if ('1. ' in sent) or ('2. ' in sent) or ('3. ' in sent) or ('4. ' in sent) or ('5. 'in sent) or ('6. 'in sent):\n",
        "        all[idx] = sent[3:]\n",
        "#제대로 안 지워진 거 다시 제거\n",
        "all_12be = []\n",
        "for text in all:\n",
        "    all_12be.append(re.sub(r'[\\n]', ' ', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "Nixtnz7LNDRV"
      },
      "outputs": [],
      "source": [
        "# from collections import Counter\n",
        "\n",
        "# #이름\n",
        "# extracted_name = []\n",
        "# for sen in all_12be:\n",
        "#     if '동지' in sen :\n",
        "#         word = sen.split(' ')\n",
        "#         for idx,char in enumerate(word):\n",
        "#             if '동지' in char:\n",
        "#                 name = char.split('동지')\n",
        "#                 if name[0] == '':\n",
        "#                     extracted_name.append(word[idx-1])\n",
        "#                 elif name[0] != '위원장':\n",
        "#                     extracted_name.append(name[0])\n",
        "\n",
        "# #이름 빈도 구하기\n",
        "# name,texts_without_name  = [],[]\n",
        "# #기존 텍스트에서 앞서 추출된 이름 제거\n",
        "# for text in all_12be:\n",
        "#     extracted_nouns, remaining_text = extract_nouns(text,extracted_name)\n",
        "#     if extracted_nouns:\n",
        "#       name.append(extracted_nouns) #기존 텍스트에서 extracted_name에서 추출된 이름과 일치하면 name에 저장\n",
        "#     texts_without_name.append(remaining_text) #extracted_name에 추출된 이름을 제거한 텍스트\n",
        "\n",
        "# word_frequencies_name = synonyms_freq(name,synonym_dict)\n",
        "# name_12be = Counter(word_frequencies_name) #마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHqu_erqNDRV"
      },
      "source": [
        "### 권력구조와 정치형태 관련 단어 사용자 정의 사전을 사용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "cjN2lc7LNDRV"
      },
      "outputs": [],
      "source": [
        "#명사 추출\n",
        "extracted_PoliticalTerms,texts_without_user_nouns  = [],[]\n",
        "for text in all_12be:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,PoliticalTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_PoliticalTerms.append(extracted_nouns)\n",
        "    texts_without_user_nouns.append(remaining_text)\n",
        "#동의어 처리\n",
        "word_frequencies_PoliticalTerms = synonyms_freq(extracted_PoliticalTerms,synonym_dict)\n",
        "\n",
        "PoliticalTerms_12be = Counter(word_frequencies_PoliticalTerms)#마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6DZU8SZNDRV"
      },
      "source": [
        "### Stopwords 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "Jymzkh9SNDRV"
      },
      "outputs": [],
      "source": [
        "texts_without_stopwords = [\n",
        "    remove_stopwords(text, stopwords).replace(\"조선로동\", \" \")\n",
        "    for text in texts_without_user_nouns\n",
        "]\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_stopwords = clean_text(texts_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqqaxrXxNDRW"
      },
      "source": [
        "### soynlp 명사 추출기 (이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTLmQf8tNDRW"
      },
      "outputs": [],
      "source": [
        "from soynlp.noun import LRNounExtractor_v2\n",
        "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
        "noun_extractor.train(texts_without_stopwords)\n",
        "soynlp_nouns = noun_extractor.extract()\n",
        "\n",
        "#soynlp로 추출된 명사 중 2글자 이상만 저장\n",
        "extracted_soynlp_words = [word for word in soynlp_nouns if len(word) >= 2]\n",
        "soynlp_12be = Counter(extracted_soynlp_words)\n",
        "\n",
        "#soynlp로 추출된 명사들을 texts_without_stopwords에서 삭제한다.\n",
        "texts_without_soynlp=[]\n",
        "for text in texts_without_stopwords:\n",
        "    remaining_text = remove_stopwords(text,extracted_soynlp_words)\n",
        "    texts_without_soynlp.append(remaining_text)\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_soynlp = clean_text(texts_without_soynlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GDgX8r0NDRW"
      },
      "source": [
        "### 북한 용어사전을 활용하여 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "ee9wuVy9NDRW"
      },
      "outputs": [],
      "source": [
        "extracted_nouns_NKterms,texts_without_extracted_nouns = [],[]\n",
        "for text in texts_without_soynlp:\n",
        "    extracted_nouns, remaining_text = extract_nouns(text,NKTerms)\n",
        "    if extracted_nouns:\n",
        "      extracted_nouns_NKterms.append(extracted_nouns)\n",
        "    texts_without_extracted_nouns.append(remaining_text)\n",
        "\n",
        "#동의어 처리\n",
        "word_frequencies_NKterms = synonyms_freq(extracted_nouns_NKterms,synonym_dict)\n",
        "NKterms_12be = Counter(word_frequencies_NKterms) # 마지막에 모든 데이터를 합칠 때, '+'를 사용하기 위해\n",
        "\n",
        "#' '와 같이 공백 삭제\n",
        "texts_without_NKterms = clean_text(texts_without_extracted_nouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHVX-NpNDRX"
      },
      "source": [
        "### Mecab 명사 추출기(이미 명사로 분류된 단어들은 제외)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWPBx2O_NDRX"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "mecab_nouns = []\n",
        "for text in texts_without_extracted_nouns:\n",
        "    mecab_nouns += mecab.nouns(text)\n",
        "\n",
        "mecab_nouns_clean = []\n",
        "for char in mecab_nouns:\n",
        "    if len(char)>1:\n",
        "        mecab_nouns_clean.append(char)\n",
        "\n",
        "mecab_nouns = [synonym(noun, synonym_dict) for noun in mecab_nouns_clean] #동의어 처리\n",
        "\n",
        "#단어 빈도수 계산\n",
        "mecab_12be = Counter(mecab_nouns)\n",
        "mecab_12be\n",
        "\n",
        "# 필요시\n",
        "# texts_without_mecab=[]\n",
        "# for text in texts_without_extracted_nouns:\n",
        "#     remaining_text = remove_stopwords(text,mecab_nouns)\n",
        "#     texts_without_mecab.append(remaining_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgD4NkxPNDRY"
      },
      "source": [
        "### 모두 합하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuegRlNLNDRY"
      },
      "outputs": [],
      "source": [
        "word_all_12be = mecab_12be+soynlp_12be+NKterms_12be+PoliticalTerms_12be\n",
        "combined_all_12be = synonyms_freq(word_all_12be, synonym_dict)\n",
        "\n",
        "combined_all_12be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxUYclp0NDRZ"
      },
      "source": [
        "### 단어 빈도 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "lBpVGOXKNDRZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font_path = '/Users/kimsuyeon/Desktop/Project2_TM/nanum-gothic/NanumGothicBold.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnP4xTdHNDRa"
      },
      "source": [
        "#### 상위 10개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIsZ7HSZNDRa"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(10)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 10개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Guog4SyNDRa"
      },
      "source": [
        "#### 상위 20개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKAUIPJHNDRa"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(20)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 20개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIa2HW0uNDRb"
      },
      "source": [
        "#### 상위 30개 단어(전문)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSCrXH86NDRb"
      },
      "outputs": [],
      "source": [
        "sorted_word_frequencies = Counter(combined_all_12be).most_common(30)\n",
        "words, frequencies = zip(*sorted_word_frequencies)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.title(\"상위 30개의 명사\", fontproperties=fontprop)\n",
        "plt.ylabel(\"빈도\", fontproperties=fontprop)\n",
        "plt.xlabel(\"명사\", fontproperties=fontprop)\n",
        "plt.xticks(rotation=45, ha='right', fontproperties=fontprop)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8MFxw5SZwyn"
      },
      "source": [
        "## 결과 엑셀로 내보내기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_word_freq = {}\n",
        "doc_word_freq['23end'] = combined_all_23end\n",
        "doc_word_freq['23be'] = combined_all_23be\n",
        "doc_word_freq['22be'] = combined_all_22be\n",
        "doc_word_freq['21be'] = combined_all_21be\n",
        "doc_word_freq['20be'] = combined_all_20be\n",
        "doc_word_freq['19be'] = combined_all_19be\n",
        "doc_word_freq['18be'] = combined_all_18be\n",
        "doc_word_freq['17be'] = combined_all_17be\n",
        "doc_word_freq['16be'] = combined_all_16be\n",
        "doc_word_freq['15be'] = combined_all_15be\n",
        "doc_word_freq['14be'] = combined_all_14be\n",
        "doc_word_freq['13be'] = combined_all_13be\n",
        "doc_word_freq['12be'] = combined_all_12be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export_to_csv(doc_word_freq, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4️⃣Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "from striprtf.striprtf import rtf_to_text\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#문서-단어 행렬로 이루어진 데이터셋 불러오기\n",
        "doc_term = pd.read_csv('/Users/kimsuyeon/Desktop/doc_word_freq.csv',index_col=0)\n",
        "#단어 추출\n",
        "terms = list(doc_term.columns)\n",
        "\n",
        "#불용어(stopwords) 추출\n",
        "with open('/Users/kimsuyeon/Desktop/remove_words.rtf', 'r', encoding='utf-8') as f:\n",
        "    rtf_content = f.read()\n",
        "remove_list = rtf_to_text(rtf_content)\n",
        "stopwords = set([word.strip() for word in remove_list.splitlines() if word.strip()])\n",
        "\n",
        "#문서별 단어리스트\n",
        "documents_words = []\n",
        "for doc in doc_term.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dictionary = corpora.Dictionary(documents_words)\n",
        "#빈도 필터링\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) # 최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출\n",
        "#Bag-of-Words 생성\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 최적의 토픽 수 찾기 (1)Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perplexity_values = []\n",
        "for i in range(2,8):\n",
        "  lda_model = LdaModel(corpus,num_topics=i,id2word=dictionary,random_state=42,passes=7,alpha=1.0,eta=0.5)\n",
        "  perplexity_values.append(lda_model.log_perplexity(corpus))\n",
        "\n",
        "plt.plot(range(2,8),perplexity_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"perplexity score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 최적의 토픽 수 찾기 (2)coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coherence_values = []\n",
        "for i in range(2,8):\n",
        "  lda_model = LdaModel(corpus,num_topics=i,id2word=dictionary,random_state=42,passes=7,alpha=1.0,eta=0.5)\n",
        "  coherence_model_lda = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,topn=10)\n",
        "  coherence_lda = coherence_model_lda.get_coherence()\n",
        "  coherence_values.append(coherence_lda)\n",
        "\n",
        "plt.plot(range(2,8),coherence_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"coherence score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 최적의 토픽 수:3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#각 문서별 토픽 분포 저장\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = doc_term.index\n",
        "df.to_excel('doc_topic_distributions.xlsx', index=False)\n",
        "\n",
        "#시각화\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df #1:국방,2:정치,3:사회"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 참고(최적의 하이퍼파라미터 찾기)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_range = range(2, 8)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = doc_term.index\n",
        "df.to_excel('doc_topic_distributions_best1_(1).xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_best1_(1).html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 시기별"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T1 = doc_term.loc[['12be', '13be', '14be', '15be', '16be']]  #2012-2016\n",
        "T2 = doc_term.loc[['17be', '18be', '19be']]                  #2017-2019\n",
        "covid_period = doc_term.loc[['20be', '21be']]                #2020-2021\n",
        "T3 = doc_term.loc[['22be', '23be','23end']]                  #2022-2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T1 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 최적의 조합 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#문서별 단어리스트\n",
        "documents_words = []\n",
        "for doc in T1.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T1.index\n",
        "df.to_excel('doc_topic_distributions_T1.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T1.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T1.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#각 문서별 토픽 분포 저장\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T1.index\n",
        "df.to_excel('doc_topic_distributions_T1.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T1.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T2 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 최적의 조합 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#문서별 단어리스트\n",
        "documents_words = []\n",
        "for doc in T2.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T2.index\n",
        "df.to_excel('doc_topic_distributions_T2.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T2.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T2.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#각 문서별 토픽 분포 저장\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T2.index\n",
        "df.to_excel('doc_topic_distributions_T2.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T2.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Covid Period (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 최적의 조합 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#문서별 단어리스트\n",
        "documents_words = []\n",
        "for doc in covid_period.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) \n",
        "#최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출(해당 기간의 텍스트 데이터셋은 매우 적으므로 하이퍼파라미터 조정이 필요)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = covid_period.index\n",
        "df.to_excel('doc_topic_distributions_covid_period.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_covid_period.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in covid_period.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=1, no_above=0.98) \n",
        "#최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출(해당 기간의 텍스트 데이터셋은 매우 적으므로 하이퍼파라미터 조정이 필요)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9#,\n",
        "    #alpha=1.0,\n",
        "    #eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#각 문서별 토픽 분포 저장\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = covid_period.index\n",
        "df.to_excel('doc_topic_distributions_covid_period.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_covid_period.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T3 (Topic Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 최적의 파라미터 조합 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#문서별 단어리스트\n",
        "documents_words = []\n",
        "for doc in T3.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "topic_range = range(2, 4)\n",
        "passes_options = range(5,101)\n",
        "#alpha_options = [0.0001 ,0.001, 0.01, 'auto']\n",
        "#eta_options = [0.0000001,0.000001,0.00001, 0.0001, 0.001, 0.01, 'auto']\n",
        "\n",
        "best_coherence = -1\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for num_topics, passes in product(topic_range, passes_options):\n",
        "    lda_model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=passes,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    coherence_model = CoherenceModel(model=lda_model,texts=documents_words,dictionary=dictionary,coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print(f\"Num Topics = {num_topics}, Passes = {passes}, Alpha = {'auto'}, Eta = {'auto'}, Coherence Score = {coherence_score}\")\n",
        "\n",
        "\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_model = lda_model\n",
        "        best_params = {'num_topics': num_topics, 'passes': passes, 'alpha': 'auto', 'eta':'auto'}\n",
        "\n",
        "\n",
        "print(\"Best Model Parameters:\")\n",
        "print(f\"Num Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}\")\n",
        "print(f\"Best Coherence Score: {best_coherence}\")\n",
        "\n",
        "\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = best_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(best_params['num_topics'])])\n",
        "df['Document'] = T3.index\n",
        "df.to_excel('doc_topic_distributions_T3.xlsx', index=False)\n",
        "\n",
        "lda_vis_best = gensimvis.prepare(best_model, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.save_html(lda_vis_best, 'lda_visualization_T3.html')\n",
        "pyLDAvis.display(lda_vis_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_words = []\n",
        "for doc in T3.values:\n",
        "    words_per_doc = []\n",
        "    for i, freq in enumerate(doc):\n",
        "        #불용어 제거 + (단어,빈도수)로 구성된 text dataset\n",
        "        if freq > 0 and terms[i] not in stopwords:\n",
        "            for _ in range(freq):\n",
        "                words_per_doc.append(terms[i])\n",
        "    documents_words.append(words_per_doc)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(documents_words)\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.90) #최소 두 번 이상, 문서의 90%를 차지하지 않는 단어 추출\n",
        "corpus = [dictionary.doc2bow(doc) for doc in documents_words]\n",
        "\n",
        "\n",
        "#LDA\n",
        "num_topics = 3\n",
        "lda_model= LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=9,\n",
        "    alpha=1.0,\n",
        "    eta=0.05\n",
        ")\n",
        "\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=documents_words, dictionary=dictionary, coherence='c_v')\n",
        "overall_coherence = coherence_model.get_coherence()\n",
        "print(f\"Overall Coherence Score: {overall_coherence}\")\n",
        "\n",
        "#각 문서별 토픽 분포 저장\n",
        "doc_topic_distributions = []\n",
        "for i, doc_bow in enumerate(corpus):\n",
        "    doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "    doc_topic_distributions.append([topic_prob for _, topic_prob in doc_topics])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(doc_topic_distributions, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
        "df['Document'] = T3.index\n",
        "df.to_excel('doc_topic_distributions_T3.xlsx', index=False)\n",
        "\n",
        "\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(lda_vis, 'lda_visualization_T3.html')\n",
        "pyLDAvis.display(lda_vis)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sSrPB41ZkChq",
        "7y1smDFVmZp5",
        "YyV_xctqtYXI",
        "D-GmzcN8FS5v",
        "Z58SFXzaHble",
        "juGOGPVOHblk",
        "KdYYiWT_JBkr",
        "bBAdl25EJcZd",
        "s5-V4lNjJEDL",
        "0K8DwGC6KNjo",
        "22tveb6HJGWW",
        "E-eVngsxM80s",
        "CSkdS5p_JIBI",
        "yTrBeE1TNCXx",
        "2PPcav8cJJul",
        "7PFdmCS5NOLP",
        "wjf6B6dTJLwB",
        "Zw-_3IjhNMk_",
        "hKdXB7B-JM6p",
        "TQSGZKVRNKx2",
        "xVqTAB8JJOcj",
        "-6fTaB7mNInb",
        "cvNUjt_cJQA_",
        "g1m4XORqNGPM",
        "0TYJKz4YJRxv",
        "O8MFxw5SZwyn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
